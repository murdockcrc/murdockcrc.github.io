<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-13T21:58:38+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Luis Delgado Blog</title><subtitle>Digitalization | Cloud | Innovation. Luis Delgado, CTO Option 4.0 AG</subtitle><entry><title type="html">Develeraging the supply chain and reducing the fragility of globalisation</title><link href="http://localhost:4000/2020/03/13/globalization-fragility.html" rel="alternate" type="text/html" title="Develeraging the supply chain and reducing the fragility of globalisation" /><published>2020-03-13T00:00:00+01:00</published><updated>2020-03-13T00:00:00+01:00</updated><id>http://localhost:4000/2020/03/13/globalization-fragility</id><content type="html" xml:base="http://localhost:4000/2020/03/13/globalization-fragility.html">&lt;p&gt;&lt;em&gt;Through off-shoring, the relentless pursuit for profit has created an unacceptable amount of systemic risk in our societies.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/content/images/2020/03/abyss-3.jpg&quot; alt=&quot;&quot; class=&quot;wrapper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The phenomenon knows as &lt;em&gt;Globalization&lt;/em&gt; accelerated and picked up pace during the 1990’s. Large corporations were finally able to deploy their capital anywhere in the world. Accelerated by international commercial treaties, de-regulation, low oil prices and support from political elites, multi-national companies started branching out of their home turfs and venturing towards towards developing nations.&lt;/p&gt;

&lt;p&gt;Across boardrooms and senior executive teams, the idea of &lt;em&gt;cost reduction&lt;/em&gt; took hold with fervor. Firms started off-shoring some of their most crucial activities, in particular manufacturing. With the reliability of industrial automation, you could produce the same goods, at (almost) the same quality anywhere in the world. Manufacturing labor started moving outwards to countries whose cost structure was much more attractive.&lt;/p&gt;

&lt;p&gt;Developing nations were armed with cheap labor and hungry for foreign investment. Developed countries were armed with capital, intellectual property, and an insatiable customer base. Off-shoring meant huge cost savings for industrial conglomerates. A massive shift took place, whereby companies invested capital in developing nations to build manufacturing facilities, brought their production know-how to far-away places, hired cheap labor, and scaled out production at a fraction of a cost compared to their home countries.&lt;/p&gt;

&lt;p&gt;This massive shift in production required a fundamental re-architecting of the supply chain, which now required a global footprint. Raw materials needed to be procured and sent over to distant places. Transportation and logistics increased their complexity by orders of magnitude. Insurance had to play a more prominent role in protecting all those billions of dollars of materials and goods as they moved from one side of the world to the next one. Complex distribution chains had to be built for this purpose.&lt;/p&gt;

&lt;p&gt;The formula proved to be successful. By putting the cost structure under pressure, profits increased. By producing at lower cost, goods were accessible to even more customers, thus increasing sales.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Lower production costs + more sales = higher stock prices and bonuses for the exec teams. Impossible to refuse&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Why produce textiles in Switzerland, when you could do it much cheaper in Honduras?
Why manufacture in Germany, if you could do it cheaper in Thailand?
Why pay the high salaries of “expensive” USA blue-collar workers, when Chinese workers could do the same job for much less?&lt;/p&gt;

&lt;p&gt;The idea was executed wonderfully and was a win-win situation. Large multi-nationals could earn more money. The customers could have access to more and cheaper goods. Developing nations increased the employment of their workforce. Developed economies transfered some of their know-how and expertise to countries which did have it.&lt;/p&gt;

&lt;p&gt;It all came crashing down with the Coronavirus.&lt;/p&gt;

&lt;p&gt;The capacity of rich nations to produce tangible goods by themselves was severely dimished throughout the last three decades. In some industries, rich countries can’t even self-sufficiently produce some very essential goods. Rich, developed nations have become overly reliant on a global supply network which, when put under stress, can become dislocated and create social pressure in affected nations.&lt;/p&gt;

&lt;p&gt;In their endless pursuit for profit, rich countries and their corporations have annihilated their ability to produce basic goods themselves:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.bloomberg.com/news/articles/2020-03-26/the-world-could-run-out-of-gloves-as-plants-curbed-in-lockdown&quot;&gt;60% of the world’s latex gloves are manufactured in Malaysia&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.reuters.com/article/us-china-usa-rareearth-refining-idUSKCN1T004J&quot;&gt;80% of rare earth demand is supplied by China&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.theguardian.com/technology/2011/oct/25/thailand-floods-hard-drive-shortage&quot;&gt;50% of the world’s hard drive supply comes from Thailand&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I am not writing this to denounce globalisation or the off-shoring of labor in general. Quite the contrary, sometimes this is needed. In certain industries, countries cannot supply the demand for specific skillsets with in-country staff. Under such conditions, having access to a larger talent pool is crucial. But when the decision to off-shore is made purely on its benefit to financial bottomlines, then problems start. If the only driver for decision-making is money-making, you can be sure that &lt;a href=&quot;https://en.wikipedia.org/wiki/Irrational_exuberance&quot;&gt;excess and risk-taking will happen at industrial scale&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’d like to call this phenomenom the &lt;em&gt;leveraging of the supply chain&lt;/em&gt;. In finance, &lt;em&gt;leverage&lt;/em&gt; is the process whereby you take capital from someone else and invest it for your own profit. Like borrowing money from the bank and investing it in the stock market. When things are good, your returns on investments will be larger than the cost to service your debt, so you make a profit. So you borrow more money to achieve higher returns. But when things go bad and you start to lose money on the market, you have both a capital loss &lt;em&gt;and&lt;/em&gt; you need to pay your debt (plus interest). Leverage is a magnifier for both profits and losses.&lt;/p&gt;

&lt;p&gt;This is the same phenomenon applied to manufacturing. When things go well, off-shoring increases your profits handsomely. But when the global supply chain is dislocated, the magnitude of these systemic shocks are more profound. As the Coronavirus crisis unfolds in the West, rich countries suddlenly found themselves in a very poor situation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Unable to produce hand sanitizer by itself, Switzerland found some of its imports being &lt;a href=&quot;https://business.financialpost.com/pmn/business-pmn/swiss-retailer-gets-stiffed-as-hand-sanitizer-seized-at-italian-border&quot;&gt;confiscated by Italian customs authorities&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Unable to provide the most essential Personal Protection Equipment (PPE) to their health care staff, US personel are forced to improvise and use anything to protect themselves&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;I’m a physician at a hospital in NYC and THIS IS THE “PPE” I WAS JUST HANDED for my shift. Our federal government has completely failed its health care workers. &lt;a href=&quot;https://twitter.com/hashtag/GetUsPPE?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#GetUsPPE&lt;/a&gt; &lt;a href=&quot;https://t.co/bEh11ra7Ee&quot;&gt;pic.twitter.com/bEh11ra7Ee&lt;/a&gt;&lt;/p&gt;&amp;mdash; Rachel Meislin, MD (@racheljulie) &lt;a href=&quot;https://twitter.com/racheljulie/status/1245114069339852802?ref_src=twsrc%5Etfw&quot;&gt;March 31, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;a href=&quot;https://www.bloomberg.com/news/videos/2020-03-27/supply-chain-collapsing-from-coronavirus-video&quot;&gt;food supply chain&lt;/a&gt; has been put at risk in many parts of the world.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usually, financial crises are caused through some type of over leverage. In the same way, I believe that we have been leveraginng the supply chains too much and we should use its coronavirus-induced dislocation as a wakeup call to de-leverage it. For example, in the same way that anti-trust regulators intervene and block the sale/merger between companies when such a deal might compromise the benefits for consumers, governments and policymakers should also exercise veto power when a company’s off-shoring might put the national supply of some critical good or service at risk. Or governments might want to incentivize a company to keep some degree of local production, at least the bare minimum needed within a specific region.&lt;/p&gt;

&lt;p&gt;This has to be done throuhg a framework of cooperation. The purpose here is not to push for &lt;em&gt;nationalisation&lt;/em&gt; or populism. What I strive to achieve here is to mitigate the risks that modern society is exposed to if most of the manufacturing capability of the world is concentrated in a few spots around the globe.&lt;/p&gt;</content><author><name></name></author><category term="careers" /><summary type="html">Through off-shoring, the relentless pursuit for profit has created an unacceptable amount of systemic risk in our societies.</summary></entry><entry><title type="html">The future of mobility is electric, but it’s not your beloved Tesla</title><link href="http://localhost:4000/2020/01/18/the-future-of-mobility-is-electric-but-its-not-your-beloved-tesla-2.html" rel="alternate" type="text/html" title="The future of mobility is electric, but it’s not your beloved Tesla" /><published>2020-01-18T20:37:12+01:00</published><updated>2020-01-18T20:37:12+01:00</updated><id>http://localhost:4000/2020/01/18/the-future-of-mobility-is-electric-but-its-not-your-beloved-tesla-2</id><content type="html" xml:base="http://localhost:4000/2020/01/18/the-future-of-mobility-is-electric-but-its-not-your-beloved-tesla-2.html">&lt;p&gt;Electric vehicles are &lt;a href=&quot;https://money.usnews.com/investing/news/articles/2020-01-13/more-than-400-000-german-jobs-at-risk-in-switch-to-electric-cars-handelsblatt&quot;&gt;disrupting the car industry&lt;/a&gt;. There is no doubt about this. They have fewer mechanical parts than their combustion engine counterparts. They don’t produce gas emissions. They are quieter. They give their owners the satisfactory feeling of being more environmentally friendly (whether that is indeed the case is heavily disputed, but let’s assume it’s true).&lt;/p&gt;

&lt;p&gt;Yet, setting our hopes of modern, environmentally-friendly mobility on electric cars is not only insufficient, it is ultimately wrong.&lt;/p&gt;

&lt;p&gt;The future is mobility is indeed electric, but not in the form of a car. Instead, it’s a bike.&lt;/p&gt;

&lt;figure class=&quot;kg-card kg-image-card kg-card-hascaption&quot;&gt;&lt;img src=&quot;/content/images/2020/01/stromer.jpg&quot; class=&quot;kg-image&quot; /&gt;&lt;figcaption&gt;This is the future of mobility. An electric bicycle. A really good-looking ebike.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;I’ve been commuting to work with an ebike. I have a relatively long commute (30-35km each way, depending on which office I have to visit). And yet, the only thing I regret about commuting with my ebike is not having done it earlier. In this post, I’d like to share with you arguments about why we should think about ebikes as a mobility solution. Just like Tesla thought out of the box when designing cars, we as a society need to think radically different and change our own expectations of mobility.&lt;/p&gt;

&lt;p&gt;Let’s look at why the ebike is a better alternative for our mobility needs:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Car battery production&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Your sexy electric car does not expel gases out of a tailpipe, but the production of its battery is not environmentally-friendly. Production of batteries require a lot of energy. They contain large amounts of rare-earth materials which are mined in a destructive way. The batteries of cars are huge, and so is the environmental footprint.&lt;/p&gt;

&lt;p&gt;It is true that electric cars &lt;a href=&quot;https://www.ucsusa.org/resources/cleaner-cars-cradle-grave#.Vv0_OhIrKRt&quot;&gt;have less emissions than conventional cars&lt;/a&gt; (accounting for production, usage and disposal of the car across its entire lifecycle). Still, the emissions produced are significant. We are still not solving the problem, we are just pushing the problem somewhere else (to the places where batteries are manufactured and to the places where our electricity is generated).&lt;/p&gt;

&lt;p&gt;The sheer size of a car make it unsustainable. By comparison, a normal ebike (the S-Pedelec variant, which reaches speeds up to 45km/h) typically has a 500Wh battery. Compare this to a Tesla Model S 85kWh battery. That’s 85000Wh. That battery is 170x bigger than that of an eBike. And let us not forget that, depending on where you are, the electricity used to charge your battery was produced by burning fossil fuels or nuclear fuel. This would also be true for an ebike battery, yet charging 500Wh vs 85kWh is a different order of magnitude.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Over-dimensioned cars&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Do you really need a car? When I commute with my ebike, I see that most of the cars which are stuck in a traffic jam are occupied only by the driver. Such big vehicles occupying so much space, yet there is only one person inside. We need to rethink this.&lt;/p&gt;

&lt;p&gt;You might be driving a modern electric car, but you will still be stuck in traffic jams. Your electric engine and your auto-pilot won’t change that. Be honest about it and realise that the majority of your mobility needs can be covered with a 1-person vehicle, and that’s what the bike is.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Range&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Electric bikes do not have the range of an electric car. But be critical about your range needs. According to the &lt;a href=&quot;https://www.bfs.admin.ch/bfs/de/home/statistiken/mobilitaet-verkehr/personenverkehr/pendlermobilitaet.html&quot;&gt;Bundesamt für Statistik&lt;/a&gt;, the average commuting distance in Switzerland is 15km (one way commute). In the US, the &lt;a href=&quot;https://itstillruns.com/far-americans-drive-work-average-7446397.html&quot;&gt;average commute distance is 16 miles&lt;/a&gt;. These distances are perfectly doable with an ebike. You don’t need a 85 kWh battery to cover your daily commute distance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Health&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This one is easy. Regularly riding an ebike will improve your health. The low-intensity effort needed to operate an ebike will improve your cardiovascular system and your muscular endurance. As of today, your Tesla won’t help you exercise.&lt;/p&gt;

&lt;p&gt;Bike commuting makes employees &lt;a href=&quot;https://www.smartcitiesdive.com/ex/sustainablecitiescollective/7-reasons-fund-bicycle-infrastructure/268971/&quot;&gt;healthier and more productive&lt;/a&gt;. They report less sick leave and higher motivational levels (less stress on the road to work, probably). In the Netherlands (a country obsessed with cycling), &lt;a href=&quot;https://ecf.com/news-and-events/news/how-dutch-love-cycling-benefitting-nation&quot;&gt;11’000 yearly deaths are prevented&lt;/a&gt; through the benefits of cycling.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Freedom and relaxation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Riding my bike while on my way to work at 0530am, I’ve been able to peacefully contemplate beautiful, orange-sky sunrises. Riding back from work at midnight, I’ve been accompanied by a sky full of stars. When riding a bike, you have the freedom to contemplate the marvellousness of the world, the beautiful landscape around, or the intricacies of the urban landscape which surrounds you. You can stop at at shop without having to look for parking spaces, or stop by a bakery for a quick coffee and snack.&lt;/p&gt;

&lt;p&gt;You can’t do any of these in car. You need to keep an eye on traffic. On the traffic lights and stop/yield signs. On the cars behind or beside you. You need to look for parking spots (and pay for them). When driving a car, your mind is too occupied in keeping up with traffic laws and avoiding a collision. True, when riding a bike you also need situational awareness, but the need for this is much less as speeds are slower and sometimes you even have the luxury of riding a bike path all by yourself.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we move from here?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We need to accept that the car is not a sustainable solution for personal mobility. Not even electric cars. Not even self-driving cars. A car is expensive to produce and its entire life cycle has an unacceptable environmental footprint. Cars produce traffic jams. Cars are expensive and stressful.&lt;/p&gt;

&lt;p&gt;As a society, we need to ditch the car and embrace the bike. We need a cultural shift and drastic policy changes. Even in a bike-friendly country like Switzerland (where I live), we are still not doing enough. Here is a list of things which I suggest we must change:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Financial incentives; financial punishment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;People need an irresistible financial incentive to embrace the ebike. In Switzerland, if you ride a bike to work, you can get 700 Swiss francs off your taxable income. This is not enough to make a difference in your tax bill. That tax break should be 10x times larger. We need to make the incentives for ebikes so attractive, that you’d be a fool to not do it.&lt;/p&gt;

&lt;p&gt;We should remove all taxes from the purchase of ebikes. This includes sales tax, import duties, and so on.&lt;/p&gt;

&lt;p&gt;We should actually PAY people to ride ebikes. Offer subsidies for them in the same way that governments subsidy the installation of solar panels at home.&lt;/p&gt;

&lt;p&gt;We also need to punish obnoxious cars with heavy taxation. Do you really need to drive a loud, noisy V6 or V8 engine? These are luxury engines which are good for nothing but polluting. A 4-cylinder engine is good enough for everyone. Yes, in my recent past, I had a powerful V6 engine too. This was an impulsive purchase and I ended up regretting it. Don’t be a fool like I was. Don’t make this mistake too.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Make the bike a first-class citizen in the roads&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;kg-card kg-image-card kg-card-hascaption&quot;&gt;&lt;img src=&quot;/content/images/2020/01/15300C60-834C-4504-863C-3DBDA3DC7717.jpeg&quot; class=&quot;kg-image&quot; /&gt;&lt;figcaption&gt;Bikes and pedestrians are second-class citizens in our cities. This is absurd. We need to change this&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;We bike riders are second-class citizens. If we are lucky, we have a little track on the right-hand side of the road for us. This track is often used by car drivers to park, or they invade this space when in a traffic jam. Cars should be treated as second-class citizens on the road. They pollute, they produce traffic jams, they are noisy. They don’t deserve their prime status in the road. Bikes should be allowed to take ownership of their lane and stop this nonsense expectation of making space for cars. If you are a car driving behind a slower bike, get used to it and wait.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Invest in more cycling infrastructure&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A long time ago, the Dutch understood the societal benefits of putting people in bikes. Investing in cycling infrastructure has huge financial benefits for an economy. In the Netherlands, 500 million Euros invested in cycling infrastructure has &lt;a href=&quot;https://www.researchgate.net/publication/280316427_Dutch_Cycling_Quantifying_the_Health_and_Related_Economic_Benefits&quot;&gt;yielded benefits worth 19 billion Euros&lt;/a&gt;. Talk about Return on Investment!&lt;/p&gt;

&lt;p&gt;Governments also need to reduce the investments in EV charging stations, and instead use that funding to invest in ebike charging solutions. It is way cheaper than charging electric cars (we need less space for the parking and much less watt hours of power).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stop the excuses, and let’s start a public dialog about sustainable mobility&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When talking to people about ebiking to work, I’ve been confronted with a plethora of excuses:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;I don’t like to sweat&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Sweat messes up my makeup in the morning&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;I don’t have access to a shower&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;I need to wear a suit to work&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;I already workout&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;It’s too cold (when in winter)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;It’s too hot (when in summer)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The helmet messes up my hair&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You need to reflect honestly and understand what the real reasons behind your reluctance are. Most of the time, they boil down to a &lt;strong&gt;lack of discipline to physically exercise&lt;/strong&gt; (even if the ebike requires low-intensity efforts) and our &lt;strong&gt;spoiled expectation of comfort inside a car&lt;/strong&gt;. These false expectations and bad habits are polluting our world, jamming up our cities, and making us fatter and unhealthier.&lt;/p&gt;

&lt;p&gt;Let’s stop that vicious circle of laziness.&lt;/p&gt;

&lt;p&gt;Get on the saddle. Pedal with comfort. Enjoy the views around you, stress-free.&lt;/p&gt;

&lt;p&gt;Make your body great again.&lt;/p&gt;

&lt;p&gt;Make our cities great again.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;And above all, let’s make our planet green again.&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><category term="mobility" /><category term="evs" /><category term="ebikes" /><summary type="html">Electric vehicles are disrupting the car industry. There is no doubt about this. They have fewer mechanical parts than their combustion engine counterparts. They don’t produce gas emissions. They are quieter. They give their owners the satisfactory feeling of being more environmentally friendly (whether that is indeed the case is heavily disputed, but let’s assume it’s true).</summary></entry><entry><title type="html">How to access local disk storage on Azure Functions for Python</title><link href="http://localhost:4000/2019/10/20/how-to-access-local-disk-storage-on-azure-functions-for-python.html" rel="alternate" type="text/html" title="How to access local disk storage on Azure Functions for Python" /><published>2019-10-20T07:52:22+02:00</published><updated>2019-10-20T07:52:22+02:00</updated><id>http://localhost:4000/2019/10/20/how-to-access-local-disk-storage-on-azure-functions-for-python</id><content type="html" xml:base="http://localhost:4000/2019/10/20/how-to-access-local-disk-storage-on-azure-functions-for-python.html">&lt;p&gt;If you try to access the local disk on Azure Functions for Python, you will be confronted with the following error:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Errno 30] Read-only file system: '/home/site/&quot;&quot;wwwroot/&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you might know from the Docs, Azure Functions for Python actually uses a container image to host your code. This Docker container is abstracted from you (you don’t have to build it or configure it in anyway). However, as a managed serverless platform, Functions does impose certain limitations, and local disk storage is one of them.&lt;/p&gt;

&lt;p&gt;In order to access the local disk, you need to fetch the &lt;em&gt;Temp&lt;/em&gt; directory on the host, like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tempfile

import azure.functions as func

def main(msg: func.ServiceBusMessage, outputblob: func.Out[str]):
    message = msg.get_body().decode('utf-8')

    dir_path = tempfile.gettempdir()
    // Save and read files on this tmp directory
    {your I/O code here}
    return
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;tempfile.gettempdir()&lt;/em&gt; will get you a reference to the folder ‘/tmp’, where you can write and read files from.&lt;/p&gt;

&lt;p&gt;Keep in mind though, that Functions is conceived as a serverless platform, and you should not depend on local disk access to do any kind of persistence. Whatever you store in that location &lt;strong&gt;will be deleted&lt;/strong&gt; on other Function invocations.&lt;/p&gt;

&lt;p&gt;In our case, we are using &lt;em&gt;pyplot&lt;/em&gt; to create some charts based on IoT data, and we use the  &lt;em&gt;dataframe.to_csv()&lt;/em&gt; method to extract some of this data into a file for further analysis. These Python modules expect a path to local disk to save their output. However, our Function will later take these local files and output them to Blob storage.&lt;/p&gt;

&lt;p&gt;Summary: if you need to access local disk on your Function app, you can do that using the &lt;em&gt;tmp&lt;/em&gt; directory. However, make sure you use local disk as temporary storage, and move those file to blob storage if you need to persist them after your Function invocation.&lt;/p&gt;

&lt;p&gt;Happy coding!&lt;/p&gt;</content><author><name></name></author><category term="azure" /><category term="cloud" /><category term="python" /><category term="functions" /><category term="azure-functions" /><summary type="html">If you try to access the local disk on Azure Functions for Python, you will be confronted with the following error:</summary></entry><entry><title type="html">No, a Swiss datacenter provider is not safer than a cloud provider</title><link href="http://localhost:4000/2019/10/05/does-a-local-data-center-protect-your-data-better.html" rel="alternate" type="text/html" title="No, a Swiss datacenter provider is not safer than a cloud provider" /><published>2019-10-05T15:53:26+02:00</published><updated>2019-10-05T15:53:26+02:00</updated><id>http://localhost:4000/2019/10/05/does-a-local-data-center-protect-your-data-better</id><content type="html" xml:base="http://localhost:4000/2019/10/05/does-a-local-data-center-protect-your-data-better.html">&lt;p&gt;I recently read a LinkedIn post from a Swiss engineer, claiming that he would never put data into any public cloud provider and instead rely on the privacy and security protection of a Swiss-based datacenter, which must also be operated by a Swiss-based legal entity.&lt;/p&gt;

&lt;p&gt;This is a frequent argument I hear in the country: they fear that the US government can access data from Swiss customers via legal tools such as the Patriot Act and the CLOUD Act. According to this point of view, the privacy of  data in the hands of an American company is not safe. Data must be hosted in a Swiss-based datacenter, operated by a Swiss company with no ties to the United States.&lt;/p&gt;

&lt;p&gt;This opinion needs to be challenged. It is misguided, misinformed and ultimately a foul play that intends to profit from people’s fear.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Is my data safer in a local datacenter, operated by a local company?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It isn’t. With the proliferation of cyberattacks in our moden society, the investments datacenter operators need to do in security far outreach their budgets. Today, the world is full of state-sponsored actors that crawl cyberspace with extremely sophisticated  tools and attack procedures. Successfully defending from such sophisticated attackers requires enourmous recurring financial investments and staffing your datacenter with highly-skilled cybersecurity specialists. Recruiting these people is extremely hard and public cloud providers are in a much better position to aquire this talent. If a sophisticated attacker wants your data, they will most certainly find their way into the data of your local datacenter.&lt;/p&gt;

&lt;p&gt;To give you an illustration about the magnitude of the problem, Microsoft invests 1 billion US dollars &lt;strong&gt;per year&lt;/strong&gt; on security (1). Your local datacenter provider does not come anywhere near that financial firepower.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What about Patriot Act and CLOUD act? Will the US government have access to my data if it is hosted in a public cloud?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a very complicated topic, but one thing is for sure: US law enforcement agencies repeatedly try to gather customer data directly from the American companies which host it. Not only that, but they typically serve a gag order to the tech company, meaning that the company cannot inform their customer that a law enforcement agency has requested their data. Eventually, this led Microsoft to repeatedly sue their own government. In Microsoft’s opinion, US agencies do not have sovereignty on data of non-American customers which resides in a datacenter outside the border of the United States. Requests to access this data for criminal investigations should follow established procedures for the exchange of information between jurisdictions. The US government disagreed and multiple lawsuits have been fought, including at the level of the Supreme Court of the United States(3).&lt;/p&gt;

&lt;p&gt;Countries can exchange information through a process called MLAT (Mutual Legal Assistance Treaty). Through the use of MLAT’s, the US has already had an instrument to get your data from your Swiss datacenter operator (this MLAT is &lt;a href=&quot;https://www.rhf.admin.ch/dam/data/rhf/strafrecht/rechtsgrundlagen/sr-0-351-933-6-e.pdf&quot;&gt;HERE&lt;/a&gt;). However, US law enforcement complained that the MLAT process is too slow, and hampers the process of gathering evidence in a criminal case.&lt;/p&gt;

&lt;p&gt;As a response to the mess created by these data requests from US law enforcement, and the difficult position tech companies had to assume, the CLOUD (Clarifying Lawful Overseas Use of Data) Act was passed in 2018. With the CLOUD act, government agencies can request data from a tech company wherever it resides, but the companies keep the right to reject a request if such a request is deemed to violate the laws of the country where the data is stored.&lt;/p&gt;

&lt;p&gt;The Zurich-based law firm Homburger produced a professional opinion about this, with the following conclusions(2):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CLOUD Act applies only to criminal cases.&lt;/li&gt;
  &lt;li&gt;It requires a court order.&lt;/li&gt;
  &lt;li&gt;It’s compatible with the Cybercrime Convention of 2012, which also applies to Switzerland.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In summary, the US government already had international legal instruments to force your local datacenter provider to hand over your data. The CLOUD Act does not make your position weaker than before.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Can we trust cloud companies to reject a request for information?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Under the CLOUD Act, companies have the right to reject a request that is deemed to violate the laws of the affected foreign country. Can we trust these companies to give us this protection? I believe so. These companies have a compelling commercial interest in protecting their customer data. It is good for their international business. They have a good record of acting in our privacy’s best interest. And they have proven to be ready to go to battle against their own government in the courts of law to protect the privacy of their customers. Microsoft publishes a report called &lt;em&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/corporate-responsibility/law-enforcement-requests-report&quot;&gt;Law Enforcement Requests Report&lt;/a&gt;&lt;/em&gt;. In there, you can see that during the second half of 2018, Microsoft rejected 17% of such requests. Facebook also produces such a report &lt;a href=&quot;https://transparency.facebook.com/government-data-requests&quot;&gt;HERE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The topic of data privacy is a complex one, and it is far from being solved. However, we must not confuse the challenges of today’s complex, interconnected world and believe that our own little datacenter is the solution to modern privacy and cybersecurity challenges. It is not, and I contend that this mentality actually exposes you to larger cybersecurity risks and offers no additional privacy protection. On top of that, refusing to use public cloud services just puts you at a disadvantage in terms of innovation capacity, agility and speed, but that’s a topic for another post :).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sources&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Brad Smith, Carol Ann Browne: “&lt;em&gt;Tools and Weapons”&lt;/em&gt;, pg. 111.&lt;/li&gt;
  &lt;li&gt;Homburger, David Rosenthal: “&lt;em&gt;Banken &amp;amp; Co. in die Cloud&lt;/em&gt;”, May 29th 2019. &lt;a href=&quot;https://media.homburger.ch/karmarun/image/upload/homburger/BJ7qKhG0V-2019-05-29_Banken%20in%20die%20Cloud_ROD.pdf&quot;&gt;https://media.homburger.ch/karmarun/image/upload/homburger/BJ7qKhG0V-2019-05-29_Banken in die Cloud_ROD.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;United States v. Microsoft Corp. &lt;a href=&quot;https://www.scotusblog.com/case-files/cases/united-states-v-microsoft-corp/&quot;&gt;https://www.scotusblog.com/case-files/cases/united-states-v-microsoft-corp/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><category term="cloud" /><category term="privacy" /><category term="law-enforcement" /><category term="switzerland" /><summary type="html">I recently read a LinkedIn post from a Swiss engineer, claiming that he would never put data into any public cloud provider and instead rely on the privacy and security protection of a Swiss-based datacenter, which must also be operated by a Swiss-based legal entity.</summary></entry><entry><title type="html">Querying time series data</title><link href="http://localhost:4000/2019/05/08/querying-time-series-data.html" rel="alternate" type="text/html" title="Querying time series data" /><published>2019-05-08T19:06:29+02:00</published><updated>2019-05-08T19:06:29+02:00</updated><id>http://localhost:4000/2019/05/08/querying-time-series-data</id><content type="html" xml:base="http://localhost:4000/2019/05/08/querying-time-series-data.html">&lt;p&gt;&lt;em&gt;This is a blog post series focused on IoT&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://blog.delgado.ms/selecting-a-database-for-iot/&quot;&gt;Part I: Introduction&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://blog.delgado.ms/ingesting-time-series-data/&quot;&gt;Part II: Ingesting data&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Part III: Querying data (this post)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s now time to start playing with the data. In this post, I’ll run some of the most common query patterns we find in IoT projects on both Azure Data Explorer and TimescaleDb.&lt;/p&gt;

&lt;p&gt;To recap: we have loaded a dataset of about 113 million rows of data to each database. For more details, read the previous blog posts (see links at the beginning of this post).&lt;/p&gt;

&lt;h2 id=&quot;lets-get-ready-to-rumble&quot;&gt;Let’s get ready to rumble&lt;/h2&gt;

&lt;h3 id=&quot;count&quot;&gt;Count&lt;/h3&gt;

&lt;p&gt;Let’s first do a count across the entire dataset.&lt;/p&gt;

&lt;p&gt;Table-wide counts are infrequent, and typically done for the internal purpose of understanding the magnitude of your dataset. However, this is a quick way to see if your data is properly indexed or if your database is keeping statistics in the background to speed up aggregation operations.&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;&lt;strong&gt;ADX&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Trucks
| count 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
      &lt;th&gt;Execution time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;114,048,366&lt;/td&gt;
      &lt;td&gt;97 ms (that’s milliseconds)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Timescale&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT count(*) FROM trucks;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
      &lt;th&gt;Execution time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;113,000,115&lt;/td&gt;
      &lt;td&gt;1 minute 01secs&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;The exact magic behind the ADX numbers are unknown to me (I’ll let the PG comment on that and enlighten us with the details), but I suspect ADX keeps some statistics or heuristics at hand to return a table-wide count in under 100 milliseconds for 114 million rows of data.&lt;/p&gt;

&lt;p&gt;On the other hand, Timescale has difficulties with this query. Running a &lt;code class=&quot;highlighter-rouge&quot;&gt;EXPLAIN ANALYZE&lt;/code&gt; on that query shows that, whereas the count at the chunk level (remember, this is where paralellisation happens) completes very fast, it’s the aggregation of the results that seems to take forever. Some people from the Timescale community have complained that Timescale on Azure Postgres suffers from slow disk performance (thanks Mario from PackIOT for the pointer), but even after increasing the IOPS on the database, the performance of the query can’t be brought down to less than 1 minute (the initial execution of this query prior to increasing the IOPS was about 3 minutes). See &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/performance-best-practices-for-using-azure-database-for-postgresql/&quot;&gt;HERE&lt;/a&gt; for the issues with disks.&lt;/p&gt;

&lt;h3 id=&quot;grouping-by-deviceid&quot;&gt;Grouping by DeviceId&lt;/h3&gt;

&lt;p&gt;Here, we’ll count how many unique DeviceId’s we have in the table, and count the number of messages per device.&lt;/p&gt;

&lt;p&gt;Counting by deviceId is important. First, it lets you quickly identify the most active devices in the field. It also lets you detect anomalies (for example, a device that has always been quiet but is now sending messages out of control). It might also be your billing unit (you charge your users on a per-message basis).&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;&lt;strong&gt;ADX&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Trucks
| summarize count() by deviceId 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/content/images/2019/05/Screenshot-2019-05-05-at-21.15.06.png&quot; alt=&quot;Screenshot-2019-05-05-at-21.15.06&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Execution time: 2.355secs&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Timescale&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT connectiondeviceid, count(*) FROM trucks GROUP BY connectiondeviceid;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/content/images/2019/05/Screenshot-2019-05-05-at-21.18.17.png&quot; alt=&quot;Screenshot-2019-05-05-at-21.18.17&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Execution time: 1min 20secs&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;
&lt;h3 id=&quot;message-count-for-1-deviceid&quot;&gt;Message count for 1 DeviceId&lt;/h3&gt;

&lt;p&gt;We will now pick one specific deviceId, and count the number of messages in the entire dataset. This is a very common metric to show your users, as it provides them a utilisation metric.&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;&lt;strong&gt;ADX&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Trucks
| where deviceId == 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'
| summarize count()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;5,698&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Execution time: 135 ms&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Timescale&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT COUNT(*) FROM trucks
WHERE connectiondeviceid = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578';
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;5,645&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1.626secs&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;We finally see Timescale coming back with a result within a reasonable timeframe. This is probably due to the low cardinality of the result set, since we are aiming for 1 single deviceId (instead of 20,000 in the past query). However, this query is complex in that it requires the database to active all its partitions. This query benefits from indexing, as can be seen by the EXPLAIN ANALYZE command:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sampledb=&amp;gt; EXPLAIN ANALYZE SELECT COUNT(*) FROM trucks WHERE connectiondeviceid = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578';
                                                                                               QUERY PLAN                                                                                                
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Aggregate (cost=6333.61..6333.62 rows=1 width=8) (actual time=7.236..7.236 rows=1 loops=1)
   -&amp;gt; Append (cost=0.00..6319.68 rows=5570 width=0) (actual time=0.052..6.938 rows=5645 loops=1)
         -&amp;gt; Seq Scan on trucks (cost=0.00..0.00 rows=1 width=0) (actual time=0.004..0.004 rows=0 loops=1)
               Filter: ((connectiondeviceid)::text = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'::text)
         -&amp;gt; Index Only Scan using _hyper_5_17_chunk_connectiondeviceid_eventprocessedutctime_idx on _hyper_5_17_chunk (cost=0.56..142.44 rows=125 width=0) (actual time=0.047..0.186 rows=114 loops=1)
               Index Cond: (connectiondeviceid = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'::text)
               Heap Fetches: 114
         -&amp;gt; Index Only Scan using _hyper_5_18_chunk_connectiondeviceid_eventprocessedutctime_idx on _hyper_5_18_chunk (cost=0.56..400.55 rows=354 width=0) (actual time=0.031..0.432 rows=360 loops=1)
               Index Cond: (connectiondeviceid = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'::text)
               Heap Fetches: 360
(shortened for brevity)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;
&lt;h3 id=&quot;get-oldest-and-newest-messages&quot;&gt;Get oldest and newest messages&lt;/h3&gt;

&lt;p&gt;We’ll now start to play around with the time dimension.&lt;/p&gt;

&lt;p&gt;In this query, we’ll get the oldest and newest message in the entire dataset. This is an unusual query to do table-wide, and it’s more frequent for it to be device-specific. More specifically, you’ll want to get the latest message sent by a device. This is important because it will tell you when was the last time the device was connected, and be able to identify devices which have not sent data in a long time and thus might be offline, dead, or in need of service. Still, I want to run this query across the entire dataset to put pressure on all the partitions and the indexes and see how they perform.&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;&lt;strong&gt;ADX&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Trucks
| summarize max(timestamp), min(timestamp)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/content/images/2019/05/Screenshot-2019-05-05-at-21.29.39.png&quot; alt=&quot;Screenshot-2019-05-05-at-21.29.39&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Execution time: 93 ms&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Timescale&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT eventprocessedutctime FROM trucks 
WHERE connectiondeviceid = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'
ORDER BY eventprocessedutctime DESC LIMIT 1;

SELECT eventprocessedutctime FROM trucks 
WHERE connectiondeviceid = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'
ORDER BY eventprocessedutctime ASC LIMIT 1;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/content/images/2019/05/Screenshot-2019-05-05-at-21.31.42.png&quot; alt=&quot;Screenshot-2019-05-05-at-21.31.42&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.134secs and 0.092secs&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;Both databases show a sub-500-millisecond performance. This shows that, when it comes to time-based queries, these databases are on fire and mean serious business.&lt;/p&gt;

&lt;h3 id=&quot;generic-time-series-query-device-specific&quot;&gt;Generic time series query, device-specific&lt;/h3&gt;

&lt;p&gt;The bread and butter of your database is going to be this query. It will return all messages sent by a specific device within a predetermined timeframe (in this case, a 4-hour time span).&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;&lt;em&gt;columns cutout for brevity&lt;/em&gt;&lt;br /&gt;
&lt;strong&gt;ADX&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;let endtime = todatetime('2019-04-14T22:29:56.28Z');
let starttime = datetime_add('hour', -4, endtime);
Trucks
| where deviceId == 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'
| where timestamp between (starttime .. endtime)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/content/images/2019/05/Screenshot-2019-05-05-at-21.35.52.png&quot; alt=&quot;Screenshot-2019-05-05-at-21.35.52&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Execution time: 778 ms&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Timescale&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT *
FROM trucks
WHERE connectiondeviceid = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'              
AND eventprocessedutctime &amp;lt;= TIMESTAMPTZ '2019-04-15 06:46:24.551888+00'
AND eventprocessedutctime &amp;gt; TIMESTAMPTZ '2019-04-15 06:46:24.551888+00' - interval '4 hours';
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/content/images/2019/05/Screenshot-2019-05-05-at-21.37.15.png&quot; alt=&quot;Screenshot-2019-05-05-at-21.37.15&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.692secs&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;
&lt;h3 id=&quot;binning&quot;&gt;Binning&lt;/h3&gt;

&lt;p&gt;Binning is the most important feature you can get in your time series database. Your IoT application will be doing time bins everywhere. It is here where most databases fail to deliver. Binning is important for two reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Significantly reduces the load on your database and on your backend application&lt;/li&gt;
  &lt;li&gt;Summarises loads of data in a way that is understandable to a human.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For those that did not read my first post in this series, binning intends to solve the following problem (if you already know what binning is, skip this paragraph). Suppose you have a device that sends a message every 5 seconds. Now, suppose the user consults the last 3 months of historical data. If you just query for all historical data for that device for the last 3 months, that query is going to return 1.5 million results. Your user won’t be able to interpret any of that data. Also, suppose you throw these 1.5 million results into a remote monitoring visualization dashboard. The graph is going to be cluttered and impossible to understand. Also, having a single query deliver 1.5 million results will put pressure on your database (in reality, you’ll have to page through results), your API, your client application and your network too. Binning allows you to do the following: “for all the data accumulated in the last 3 months, split the data into bins of 1 day. Calculate the average of the measurements on every bin, and return the result to the user”. Of course, you can define the length of the bins and the aggregation function you want to use.&lt;/p&gt;

&lt;p&gt;We are now going to run binning on both ADX and Timescale. We will do that for 1 deviceId, for a period of 4 hours, with bins of 30 minutes, and we’ll compute the average, min value and max value on every bin. We’ll be looking at the temperature value. But before we do that, we’ll simply count the number of messages for that period of time, just to get an idea about the size of the result set without binning.&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;&lt;strong&gt;ADX&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;let endtime = todatetime('2019-04-14T22:29:56.28Z');
let starttime = endtime - time(4h);
let interval = 30min;
Trucks
| where deviceId == 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'
| where timestamp between (starttime .. endtime)
| summarize count() 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1,440&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Execution time: 0.132secs&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Timescale&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT count(*)
FROM trucks
where connectiondeviceid = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'              
    and eventprocessedutctime &amp;lt;= TIMESTAMPTZ '2019-04-15 06:46:24.551888+00'
    and eventprocessedutctime &amp;gt; TIMESTAMPTZ '2019-04-15 06:46:24.551888+00' - interval '4 hours'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;883&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7.091secs&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;We’ll later run binning across millions of rows, don’t worry. Let’s run the binning for this one device now:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;&lt;strong&gt;ADX&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;let endtime = todatetime('2019-04-14T22:29:56.28Z');
let starttime = endtime - time(4h);
let interval = 30min;
Trucks
| where deviceId == 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'
| where timestamp between ( starttime .. endtime )
| summarize avg(temperature), min(temperature), max(temperature) by bin(timestamp, interval)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Result&lt;br /&gt;
 &lt;img src=&quot;/content/images/2019/05/Screenshot-2019-05-08-at-19.09.57.png&quot; alt=&quot;Screenshot-2019-05-08-at-19.09.57&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Execution time: 640 ms&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Timescale&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT time_bucket('30 minutes', eventprocessedutctime) AS thirty_min, avg(temperature), min(temperature), max(temperature)
FROM trucks
where connectiondeviceid = 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'              
    and eventprocessedutctime &amp;lt;= TIMESTAMPTZ '2019-04-15 06:46:24.551888+00'
    and eventprocessedutctime &amp;gt; TIMESTAMPTZ '2019-04-15 06:46:24.551888+00' - interval '4 hours'
GROUP BY thirty_min
ORDER BY thirty_min DESC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Result&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;       thirty_min | avg | min | max   
------------------------+---------------------+--------+--------
 2019-04-15 06:30:00+00 | 31.2952622950819672 | 29.969 | 32.406
 2019-04-15 06:00:00+00 | 34.4474181818181818 | 31.084 | 37.441
 2019-04-15 05:30:00+00 | 35.4402181818181818 | 33.909 | 37.544
 2019-04-15 05:00:00+00 | 36.2109636363636364 | 34.097 | 37.976
 2019-04-15 04:30:00+00 | 35.0598288288288288 | 33.891 | 37.511
 2019-04-15 04:00:00+00 | 34.0196576576576577 | 33.018 | 35.255
 2019-04-15 03:30:00+00 | 35.0200181818181818 | 33.726 | 36.065
 2019-04-15 03:00:00+00 | 35.9602252252252252 | 34.325 | 37.834
 2019-04-15 02:30:00+00 | 35.3813061224489796 | 34.445 | 36.455
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Execution time: 221 ms&lt;/p&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;Now let’s remove the filter on deviceId and see how binning behaves across millions of rows of data:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;&lt;strong&gt;ADX&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;let endtime = todatetime('2019-04-14T22:29:56.28Z');
let starttime = endtime - time(4h);
let interval = 30min;
Trucks
//| where deviceId == 'd1626544-acfc-44fd-978a-6879f345a0da.truck-01.11578'
| where timestamp between ( starttime .. endtime )
| summarize avg(temperature), min(temperature), max(temperature) by bin(timestamp, interval)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Execution time: 635 ms&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Timescale&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT time_bucket('30 minutes', eventprocessedutctime) AS thirty_min, avg(temperature), min(temperature), max(temperature) FROM trucks where eventprocessedutctime &amp;lt;= TIMESTAMPTZ '2019-04-15 06:46:24.551888+00' and eventprocessedutctime &amp;gt; TIMESTAMPTZ '2019-04-15 06:46:24.551888+00' - interval '4 hours' GROUP BY thirty_min ORDER BY thirty_min DESC;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Execution time: 5.508 secs&lt;/p&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;If your database does not offer binning support, you’ll have to retrieve the entire result set from your database, and do the bins in your own application code. You’ll never achieve the performance seen above. Plus, we are lazy developers, we don’t want to write code unless we absolutely have to, right?&lt;/p&gt;

&lt;figure class=&quot;kg-card kg-image-card&quot;&gt;&lt;img src=&quot;/content/images/2019/05/i-choose-a-lazy-person-to-do-a-hard-because-6506353.png&quot; class=&quot;kg-image&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;One drawback of binning is that it will smooth out the data and it will be hard to spot anomalies. That’s why I typically provide binning with more than 1 dimension. In the case above, I am computing the average, the min and the max values. The average value will allow you to understand the trend of your devices. Whereas the min and max values will help you spot anomalies. You can use other aggregation functions (such as standard deviation) to dig into the data. In any case, if you spot a specific bin which looks funny, you can always query the entire dataset for &lt;strong&gt;that&lt;/strong&gt; specific bin only (remove the aggregations).&lt;/p&gt;

&lt;p&gt;For your benefit, this is how the query above would look like to a user with binning:&lt;/p&gt;

&lt;figure class=&quot;kg-card kg-image-card&quot;&gt;&lt;img src=&quot;/content/images/2019/05/Screenshot-2019-05-08-at-19.11.42.png&quot; class=&quot;kg-image&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;And without binning:&lt;/p&gt;

&lt;figure class=&quot;kg-card kg-image-card kg-card-hascaption&quot;&gt;&lt;img src=&quot;/content/images/2019/05/Screenshot-2019-05-06-at-21.34.31.png&quot; class=&quot;kg-image&quot; /&gt;&lt;figcaption&gt;Even the database itself tells you that this query is a BAD idea...&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I hope to have demonstrated some of the most common IoT data access patterns we find in our projects, as well as how to implement them using Azure Data Explorer and Timescale.&lt;/p&gt;

&lt;p&gt;Trying to implement these queries on large datasets on databases which are not optimized for time series is costly, suffers from slow performance, and takes more development effort. Therefore, you should invest time in selecting the right database for your IoT use case.&lt;/p&gt;

&lt;p&gt;I want to finish with a note about ADX and Timescale. The purpose of these blog posts &lt;strong&gt;is not&lt;/strong&gt; to compare ADX vs Timescale and decide which one is faster/better/etc. So don’t take these posts as a showdown between the two. However, people have asked what are the advantages of one over the other, or what the selection criteria are to choose between them.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Timescale&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In essence, Timescale is still an RDBMS. It builds on top of PostgreSQL, which is a relational database. Wait a minute… didn’t I say in my first post to avoid RDBMS for IoT scenarios? In fact I did, but Timescale is an optimization for time series on top of PostgreSQL. However, by looking at the performance of some of the queries above, you can see that sometimes the limitations of the relational architecture show up. The performance for counting is terrible. Typically, aggregations will perform lower compared to a database like ADX. Timescale users will usually create aggregation views on top of the telemetry tables they use to accelerate performance. When it comes to time series data, PostgreSQL is a huge improvement compared to the incumbents, such as MS SQL, Oracle or the likes. But it won’t match the performance and capabilities of a hyperscale architecture like ADX. Postgres will run on-prem, for example, if you need to keep a database at the edge for a historian or another purpose where cloud connectivity is a problem or an overkill. You can also benefit from the fact that it’s a relational database, meaning you can have other tables in the database and join them with telemetry data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ADX&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ADX uses a completely different architecture from relational databases, and you can see what that means in performance. Simply put, the performance of ADX is criminal. Specially when it comes to aggregations across billions of rows, I’ve seen ADX perform in datasets much much larger than the 113 million rows I used for these blog posts, and its performance never ceases to amaze me. It’s a product built for hyperscale and hyperperformance. It’s a product built for a scale your own organisation will probably never see (as a matter of fact, the product’s origins inside Microsoft date back to the need of handling all the telemetry and diagnostics data Azure services were generating. A problem of apocalyptic scale, considering the scale of Azure). ADX is not for everybody though. First, ADX is propietary IP from Microsoft, which means it runs in the Azure cloud only. If your organization already has an investment and commitment to Azure, then that’s alright. But if you’re invested in another cloud, or still want to run on-prem, ADX is not the right database for you.  ADX also requires your team to learn another query language (KQL - Kusto Query Language). The language is super intuitive and delightful to use, yet most people are already used to SQL and adopting ADX means your team will have to go through a slight learning curve.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Again, many thanks to the wonderful people at the Microsoft ADX team for their comments, feedback, and suggestions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Also, many thanks to the people in the community of Timescale. In particular, they have a quite active Slack group, which is public and open to anybody, where people contribute content, ideas and help each other solve problems.&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><category term="iot" /><category term="azure" /><category term="databases" /><category term="time-series" /><summary type="html">This is a blog post series focused on IoT</summary></entry><entry><title type="html">Ingesting time series data</title><link href="http://localhost:4000/2019/05/04/ingesting-time-series-data.html" rel="alternate" type="text/html" title="Ingesting time series data" /><published>2019-05-04T17:14:05+02:00</published><updated>2019-05-04T17:14:05+02:00</updated><id>http://localhost:4000/2019/05/04/ingesting-time-series-data</id><content type="html" xml:base="http://localhost:4000/2019/05/04/ingesting-time-series-data.html">&lt;p&gt;&lt;em&gt;This is a blog post series focused on IoT&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://blog.delgado.ms/selecting-a-database-for-iot/&quot;&gt;Part I: Introduction&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Part II: Ingesting data (this post)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://blog.delgado.ms/querying-time-series-data/&quot;&gt;Part III: Querying data&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-dataset&quot;&gt;The dataset&lt;/h2&gt;

&lt;p&gt;The dataset consists of simulated messages sent from trucks. Each message looks like this:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;latitude&quot;: 47.445301,
    &quot;longitude&quot;: -122.296307,
    &quot;speed&quot;: 30,
    &quot;speed_unit&quot;: &quot;mph&quot;,
    &quot;temperature&quot;: 38,
    &quot;temperature_unit&quot;: &quot;F&quot;,
    &quot;EventProcessedUtcTime&quot;: &quot;2019-04-14T05:50:34.3338468Z&quot;,
    &quot;PartitionId&quot;: 0,
    &quot;EventEnqueuedUtcTime&quot;: &quot;2019-04-14T05:49:38.9890000Z&quot;,
    &quot;IoTHub&quot;: {
        &quot;MessageId&quot;: null,
        &quot;CorrelationId&quot;: null,
        &quot;ConnectionDeviceId&quot;: &quot;truck-01.659&quot;,
        &quot;ConnectionDeviceGenerationId&quot;: &quot;636908177760737590&quot;,
        &quot;EnqueuedTime&quot;: &quot;2019-04-14T05:49:38.8800000Z&quot;,
        &quot;StreamId&quot;: null
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;You can see that the messages have been enriched with some additional properties from Azure IoT Hub, but in essence, the messages are JSON objects with information about lat/lon, speed and temperature.&lt;/p&gt;

&lt;p&gt;The dataset consits of about 114 million messages.&lt;/p&gt;

&lt;h2 id=&quot;never-stick-your-head-in-front-of-the-fire-hose&quot;&gt;Never stick your head in front of the fire hose&lt;/h2&gt;

&lt;p&gt;As a general principle, you should never deploy your custom code in between your ingestion pipeline. This is mostly because in IoT projects, you will typically have thousands of devices sending data at the same time, which makes your ingestion pipeline look like a large firehose of messages. Inserting your custom code in the middle of the firehose requires you to engineer your code to account for large scalability, paralellization, no-message loss, and many other problems which are hard to solve.&lt;/p&gt;

&lt;figure class=&quot;kg-card kg-image-card kg-card-hascaption&quot;&gt;&lt;img src=&quot;/content/images/2019/05/firehose.jpg&quot; class=&quot;kg-image&quot; /&gt;&lt;figcaption&gt;Don't stick your head in front of this. Let your cloud services handle ingestion out of the box and let them handle the problem of paralellization, throughput and message processing.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Avoiding custom code in your pipeline is sometimes not possible. Sometimes you need to transform the messages, decompress/debatch them, or enrich them with data from other systems. In such cases you’ll have no option but to stick your head in front of the firehose. But for the purposes of this exercise, we will try to avoid doing that. We’ll try to implement our ingestion with as much out-of-the-box as possible.&lt;/p&gt;

&lt;h2 id=&quot;database-setup&quot;&gt;Database setup&lt;/h2&gt;

&lt;h3 id=&quot;azure-data-explorer&quot;&gt;Azure Data Explorer&lt;/h3&gt;

&lt;p&gt;I will be running the steps in this blog post (and the next one, where I’ll deal with queries) on a shared ADX cluster. This is a cluster provided by the ADX engineering team to me for free. I don’t know how much capacity it has, all I know is that this is a cluster shared with other &lt;em&gt;tenants,&lt;/em&gt; and I’ve been advised by the engineering team to not expect too much performance out of it. So I will be assuming the performance of this cluster will be similar to the lowest-priced ADX cluster (~300 USD/month) or worse.&lt;/p&gt;

&lt;h3 id=&quot;timescale&quot;&gt;Timescale&lt;/h3&gt;

&lt;p&gt;I am running Timescale on an Azure Database for PostgreSQL. It is running 4 vCores and has a total storage capacity of 106Gb. Assuming the ADX cluster is on the low-price tier (2 vCPU D11 v2 instances, which is the lowest price entrypoint at ~300 USD/month), I am scaling PostgreSQL to a similar price point of 300 bucks.&lt;/p&gt;

&lt;p&gt;Both PostgreSQL and ADX are offered as platform-as-a-service from Microsoft and are fully managed by Microsoft. I don’t have to deal with any infrastructure problems and they can be provisioned automatically via ARM in minutes. I do not have to manage any storage, networking or VMs for any of the 2 databases.&lt;/p&gt;

&lt;h2 id=&quot;ingesting-with-azure-data-explorer&quot;&gt;Ingesting with Azure Data Explorer&lt;/h2&gt;

&lt;p&gt;Azure Data Explorer (ADX for short) provides several out of the box features to ingest data. For details about these ingestion methods, see &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/data-explorer/ingest-data-overview&quot;&gt;HERE&lt;/a&gt;. All my data is exported into large blob files, so I used ADX’s blob ingest feature to bulk-import the data.&lt;/p&gt;

&lt;h3 id=&quot;preparing-the-adx-database&quot;&gt;Preparing the ADX database&lt;/h3&gt;

&lt;p&gt;ADX works with tables. So the first thing I had to do was to create a table for all my &lt;em&gt;Truck&lt;/em&gt; data:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.create tables Trucks (deviceId:string, timestamp:datetime, latitude:float, longitude:float, speed:float, speed_unit:string, temperature:float, temperature_unit:string)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;JSON messages don’t fit tabular formats well. So the next step is to tell ADX how to map my JSON messages into the table above. ADX does this by implementing &lt;strong&gt;table mappings:&lt;/strong&gt;&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.create table Trucks ingestion json mapping 'TrucksMapping' '[{&quot;column&quot;:&quot;deviceId&quot;,&quot;path&quot;:&quot;$.IoTHub.ConnectionDeviceId&quot;}, {&quot;column&quot;:&quot;timestamp&quot;,&quot;path&quot;:&quot;$.IoTHub.EnqueuedTime&quot;}, {&quot;column&quot;:&quot;latitude&quot;,&quot;path&quot;:&quot;$.latitude&quot;}, {&quot;column&quot;:&quot;longitude&quot;,&quot;path&quot;:&quot;$.longitude&quot;}, {&quot;column&quot;:&quot;speed&quot;,&quot;path&quot;:&quot;$.speed&quot;}, {&quot;column&quot;:&quot;speed_unit&quot;,&quot;path&quot;:&quot;$.speed_unit&quot;}, {&quot;column&quot;:&quot;temperature&quot;,&quot;path&quot;:&quot;$.temperature&quot;}, {&quot;column&quot;:&quot;temperature_unit&quot;,&quot;path&quot;:&quot;$.temperature_unit&quot;}]'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;&lt;em&gt;Update: for production scenarios, use data type&lt;/em&gt; double &lt;em&gt;instead of&lt;/em&gt; float_. This is because_ float &lt;em&gt;is a 32-bit number, whereas&lt;/em&gt; double &lt;em&gt;utilises 64 bits.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With this configuration, every time I push a JSON message from a truck, ADX will automatically flatten it into the table format I defined for the &lt;em&gt;Trucks&lt;/em&gt; table.&lt;/p&gt;

&lt;p&gt;To ingest the data, I used ADX’s native &lt;em&gt;ingest&lt;/em&gt; command. This command takes a path to Azure blob storage, parses it and ingests it. Coupled with the &lt;em&gt;async&lt;/em&gt; command, the database will immediately return an &lt;em&gt;operationId&lt;/em&gt;, which you can use to consult the status of the import operation while it runs in the background:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.ingest async into table Trucks (
  h'https://luisdeltrucks.blob.core.windows.net/trucks/0_742175f33d85471281626b9816bd6f64_1.json',
    h'https://luisdeltrucks.blob.core.windows.net/trucks/0_b32e2f3120d84ecf926539c9054da9bd_1.json')
    with (jsonMappingReference = &quot;TrucksMapping&quot;)
    with (format = &quot;json&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;Note that, by default, ADX enforces a &lt;strong&gt;1 hour timeout&lt;/strong&gt; on control commands. &lt;em&gt;Ingest&lt;/em&gt; is a control command. This timeout is not extendable. Therefore, if your data is large and the import process takes more than 1 hour to complete, you’ll have to split your dataset into multiple files like I did above.&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Ingested rows&lt;/td&gt;
      &lt;td&gt;114.5 million&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data size (sum of blob size)&lt;/td&gt;
      &lt;td&gt;55.3 GB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ingestion time&lt;/td&gt;
      &lt;td&gt;1hour 9minutes&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;
&lt;h2 id=&quot;ingesting-with-timescale&quot;&gt;Ingesting with Timescale&lt;/h2&gt;

&lt;p&gt;Since Timescale runs on top of PostgreSQL, and I vouched for platform-as-a-service in the previous post, I decided to use Timescale on Azure as a managed service. You can find the instructions to provision a Timescale db instance on Azure &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/power-iot-and-time-series-workloads-with-timescaledb-for-azure-database-for-postgresql/&quot;&gt;HERE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Timescale also offers some features to ingest data (see &lt;a href=&quot;https://docs.timescale.com/v1.2/getting-started/migrating-data&quot;&gt;HERE&lt;/a&gt;). Unfortunately none of these methods allow me to import JSON data from blob storage (you do have a feature to import CSV files, but since I don’t have my data in that format, I could not use it). So in the case of Timescale, I did have to write some code and stick my head in front of the firehose.&lt;/p&gt;

&lt;p&gt;Once I provisioned my Timescale instance, I created a hypertable for Trucks data:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CREATE TABLE Trucks
(
    latitude numeric(9,6) NOT NULL,
    longitude numeric(9,6) NOT NULL,
    speed integer NOT NULL,
    speed_unit varchar(5) NOT NULL,
    temperature numeric(6,3) NOT NULL,
    temperature_unit varchar(2) NOT NULL,
    EventProcessedUtcTime TIMESTAMPTZ NOT NULL,
    ConnectionDeviceId varchar(200) NOT NULL
);

SELECT create_hypertable('trucks', 'eventprocessedutctime', chunk_time_interval =&amp;gt; interval '1 hour');
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;Finally, I wrote this Python code to handle the bulk-import of data:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/murdockcrc/timeseries-loader/blob/f4e8429629ca10c52678a591e12e36eb8b69539a/timescale/timescale.py&quot;&gt;https://github.com/murdockcrc/timeseries-loader/blob/f4e8429629ca10c52678a591e12e36eb8b69539a/timescale/timescale.py&lt;/a&gt;&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt; &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Ingested rows&lt;/td&gt;
      &lt;td&gt;113.7 million&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Data size&lt;/td&gt;
      &lt;td&gt;52.8 Gb&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ingestion time&lt;/td&gt;
      &lt;td&gt;2hours 15minutes&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;&lt;em&gt;Note that the dataset in Timescale is slightly smaller than in ADX. This is because I imported a small data file into ADX which I did not import into Timescale. Later it was too late to correct this error and I did not want to re-run the data ingestion. However, the datasets have more or less the same sizes and this minor variation should not make a big difference.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Timescale took almost twice the time to import the dataset. This is a good illustration about why you should not stick your head in front of the firehose. I suppose Microsoft’s native blob storage import is signficantly more efficient than my Python code.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(I’m sure Microsoft has invested much more engineering time  in making the blob import, compared to the time it took me to write this Python code :). So if you have a suggestion on how to improve the timescale.py script, open up a PR in GitHub or drop a comment below).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update: Best practice for large-blob ingestion. My dataset consists of 2 large blob files: one is 47GB in size and the other is 8.1GB. Databases will have problems crunching through such large files. Specifically to ADX, the system will assign only 1 core to the ingest process per file. Therefore, issuing multiple&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ingest&lt;/code&gt; &lt;em&gt;commands is much more efficient. The same problem plagues the Timescale import, as my Python script needs to parse the large files sequentially. During the import process, the Postgres instance never achieved more than 30% resource utilisation, so I estimate the system could have processed 3 files concurrently to achieve max resource utilization. As a rule of thumb, try to make your files 1 GB in size.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-do-distributed-databases-scale&quot;&gt;How do distributed databases scale?&lt;/h2&gt;

&lt;p&gt;In the previous post, I mentioned that time series databases like ADX or Timescale perform better than traditional RDBMS. What is the magic that makes this happen? There is a lot of engineering work being done behind the covers, but in essence, the reason is &lt;em&gt;partitioning.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Partitioning is a concept whereby a database splits its dataset into pieces and stores the pieces as individual entities. Different compute nodes will hold one or more of these partitions. The partitioning is happening under the covers and is abstracted from the user (aka the application). The application asks for the data it needs, and the database is responsible for identifying the partitions on which the entire result set of the query lives. In essence, partitioning allows databases to benefit from paralellization techniques which boost query performance.&lt;/p&gt;

&lt;h3 id=&quot;how-does-adx-distribute-data&quot;&gt;How does ADX distribute data?&lt;/h3&gt;

&lt;p&gt;ADX partitions data on &lt;em&gt;Extents&lt;/em&gt;. ADX decides how to split the data into extents. In our case, the data for the &lt;em&gt;Trucks&lt;/em&gt; table has been split into 3 extents only. This decision was made by the database itself:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.show table Trucks extents 
| project ExtentId, TableName, OriginalSize, ExtentSize, CompressedSize, IndexSize
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;
&lt;!--kg-card-begin: markdown--&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ExtentId&lt;/th&gt;
      &lt;th&gt;TableName&lt;/th&gt;
      &lt;th&gt;OriginalSize&lt;/th&gt;
      &lt;th&gt;ExtentSize&lt;/th&gt;
      &lt;th&gt;CompressedSize&lt;/th&gt;
      &lt;th&gt;IndexSize&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ba89f060-03f0-41d5-9417-fe1d2f4cf80e&lt;/td&gt;
      &lt;td&gt;Trucks&lt;/td&gt;
      &lt;td&gt;3404972669&lt;/td&gt;
      &lt;td&gt;877430252&lt;/td&gt;
      &lt;td&gt;811758880&lt;/td&gt;
      &lt;td&gt;65671372&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6d3ae8c4-ba81-4bd1-a5b9-6e6a20a206df&lt;/td&gt;
      &lt;td&gt;Trucks&lt;/td&gt;
      &lt;td&gt;4129668544&lt;/td&gt;
      &lt;td&gt;1050480421&lt;/td&gt;
      &lt;td&gt;964886304&lt;/td&gt;
      &lt;td&gt;85594117&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;633b4c4d-6b58-41c4-a888-c800af5a23f3&lt;/td&gt;
      &lt;td&gt;Trucks&lt;/td&gt;
      &lt;td&gt;4149010294&lt;/td&gt;
      &lt;td&gt;1066112399&lt;/td&gt;
      &lt;td&gt;984692640&lt;/td&gt;
      &lt;td&gt;81419759&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;&lt;strong&gt;A word on data compression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pay attention to ExtentId &lt;em&gt;633b4c4d-6b58-41c4-a888-c800af5a23f3&lt;/em&gt;. Its &lt;em&gt;OriginalSize&lt;/em&gt; is reported as 4.1GB. However, &lt;em&gt;CompressedSize&lt;/em&gt; is just 984MB. Index size is 81MB. Compression ratio is slightly above 4:1. This means that the database helps you save storage costs. Although storage is typically regarded as very cheap in the cloud, on large-scale IoT scenarios (where you might be ingesting terabytes of data per day) your storage costs will add up. Compression is your friend.&lt;/p&gt;

&lt;p&gt;The other benefit of having efficient compression is that you can afford to have verbose messages without having to pay a price in terms of storage costs. Verbose messages are good. Verbosity increases the self-explanatory characteristics of messages and makes them easier to understand. It also increases the message size. With ADX, I’ve seen very verbose messages achieve a compression of 30:1, which means you can have messages with extended verbosity and still be able to store them in a cost-effective way.&lt;/p&gt;

&lt;h3 id=&quot;how-does-timescale-distribute-data&quot;&gt;How does Timescale distribute data?&lt;/h3&gt;

&lt;p&gt;Timescale also implements partitioning. The name for it is &lt;em&gt;chunk.&lt;/em&gt; By default, chunks are based on the timestamp of the messages and they will consist of periods of 7 days. Given that my test dataset was generated within a period of 24 hours, using Timescale’s default chunking configuration would have meant that all messages would have landed in the same partition. For that reason, I specified a custom chunk time interval while creating the hypertable (see code above). Timescale created 1-hour chunks of data:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;select count(*) from show_chunks();

25
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;Our 113+ million rows of data are spread across 25 chunks. This is a bit unfair towards ADX, since ADX is using only 3 extents, whereas Timescale can distritubte the load across more partitions. However, I get the chance of specifying the chunk sizing with Timescale, but I can’t do the same with ADX.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update: turns out you can control sharding in ADX too. However, the docs don’t include this content yet (the ADX team will be updating this). There are 2 policies you can play with:&lt;/em&gt; Sharding &lt;em&gt;and&lt;/em&gt; Merge_. Depending on how these policies are configured, the database may merge your data into fewer extents. This is the reason why I only get 3 extents in my dataset, as the default policy cover a max time range of 8 hours (my data was produced in a 24-hour period)._&lt;/p&gt;

&lt;p&gt;On Timescale, we can also see an efficient usage of storage:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SELECT table_name, table_size, index_size FROM timescaledb_information.hypertable;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;table_name&lt;/th&gt;
      &lt;th&gt;table_size&lt;/th&gt;
      &lt;th&gt;index_size&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;trucks&lt;/td&gt;
      &lt;td&gt;14 GB&lt;/td&gt;
      &lt;td&gt;13 GB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;Below is a comparison between ADX and Timescale ingestion:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Database&lt;/th&gt;
      &lt;th&gt;Ingested Rows&lt;/th&gt;
      &lt;th&gt;Ingestion Time&lt;/th&gt;
      &lt;th&gt;Table Size&lt;/th&gt;
      &lt;th&gt;Index Size&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ADX&lt;/td&gt;
      &lt;td&gt;114,048,366&lt;/td&gt;
      &lt;td&gt;1h 09m&lt;/td&gt;
      &lt;td&gt;2.76 GB&lt;/td&gt;
      &lt;td&gt;232 MB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Timescale&lt;/td&gt;
      &lt;td&gt;113,000,115&lt;/td&gt;
      &lt;td&gt;2h 15min&lt;/td&gt;
      &lt;td&gt;14 GB&lt;/td&gt;
      &lt;td&gt;13 GB&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;The original size of the dataset to ingest (as measured by the size of the blobs where the JSON messages were stored) is 55.3GB. Both Timescale and ADX managed to significantly reduce that size by using aggressive compression. ADX does manage to squeeze in more messages per unit of storage, which is clearly demonstrated by the differences in the size of the both the tables and the indexes.&lt;/p&gt;

&lt;h3 id=&quot;data-indexing&quot;&gt;Data Indexing&lt;/h3&gt;

&lt;p&gt;ADX will, by default, &lt;a href=&quot;https://azure.microsoft.com/en-us/blog/azure-data-explorer-technology-101/&quot;&gt;index everything&lt;/a&gt; which is a string or dynamic column.&lt;/p&gt;

&lt;p&gt;Timescale will, by default, index the column used to partition data in the hyper table:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sampledb=&amp;gt; SELECT * FROM indexes_relation_size('trucks');
 public.trucks_eventprocessedutctime_idx | 4090363904
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;From the query above, you can see that our Timescale table has only one index.&lt;/p&gt;

&lt;p&gt;Later we will be running queries against the databases. Most of these queries will have to deal with device identifiers. For that purpose, I created an index on column &lt;em&gt;connectiondeviceid.&lt;/em&gt;&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sampledb=&amp;gt; create index on trucks (connectiondeviceid, eventprocessedutctime DESC);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;This additional index brings the total index size up from 3.9 GB to 13 GB. Certainly, you’ll need to spend some time designing your indexes with Timescale. On ADX, indexes are automatic and I don’t have to spend time on them.&lt;/p&gt;

&lt;p&gt;In the next post, we’ll look at running queries against the databases and see how they perform.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update: thanks to the ADX team, who already pointed out some optimizations possible for the ingestion process. I have incorporated these into the content of the post in the relevant sections.&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><category term="iot" /><category term="time-series" /><category term="databases" /><category term="nosql" /><summary type="html">This is a blog post series focused on IoT</summary></entry><entry><title type="html">Selecting a database for IoT</title><link href="http://localhost:4000/2019/04/22/selecting-a-database-for-iot.html" rel="alternate" type="text/html" title="Selecting a database for IoT" /><published>2019-04-22T19:23:52+02:00</published><updated>2019-04-22T19:23:52+02:00</updated><id>http://localhost:4000/2019/04/22/selecting-a-database-for-iot</id><content type="html" xml:base="http://localhost:4000/2019/04/22/selecting-a-database-for-iot.html">&lt;p&gt;&lt;em&gt;This is a blog post series focused on IoT&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Part I: Introduction (this post)&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://blog.delgado.ms/ingesting-time-series-data/&quot;&gt;Part II: Ingesting data&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://blog.delgado.ms/querying-time-series-data/&quot;&gt;Part III: Querying data&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Selecting a database which is fit for IoT scenarios is a difficult task to do. It is particularly problematic for data architects with long experience with a particular database engine. Yet, just because you’ve been using a hammer all your life doesn’t mean that every problem you face is a nail. Selecting a proper database for your use case requires open-mindedness and exploring options outside of your traditional comfort zone.&lt;/p&gt;

&lt;p&gt;In my experience, a database for IoT should at least have the following characteristics:&lt;/p&gt;

&lt;h2 id=&quot;optimised-for-large-scale-time-series-reads&quot;&gt;Optimised for large-scale time series reads&lt;/h2&gt;

&lt;p&gt;Access to your data is typically in the form of a time series. What is a time series, and why is it different from other forms of data sets?&lt;/p&gt;

&lt;p&gt;Time series describes the changes of a system across time. This is a very important characteristic of IoT data, and it will deeply impact our selection of a database. Time series data is characterised by the following three components (more &lt;a href=&quot;https://docs.timescale.com/v1.2/introduction/time-series-data&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Time-centric: time will be the main dimension on which you will be querying your data. The correctness of your result set will be dependent on the accuracy of the timestamps you have provided to your messages. For more on this topic, please see &lt;a href=&quot;https://blog.delgado.ms/5-ways-to-better-manage-datetime-in-iot-solutions/&quot;&gt;THIS&lt;/a&gt; other blog post.&lt;/li&gt;
  &lt;li&gt;Append-only: IoT data is a perfect fit for event sourcing. More on that later.&lt;/li&gt;
  &lt;li&gt;Sensitivity to latest events: users will typically care more about the time series data for the present and the most recent past. The older data gets, the less valuable it is for the user. Think about it in this way: will you care to know the temperature of a sensor right now? What about that same value, 2 years ago? Data which is far away in the past is typically only useful for data scientists trying to run experiments on historical data. For the normal IoT user, old data has less value. This might sound trivial, but it will play a key role in the cost of your solution. This is because it does not make sense for you to store all your data in expensive storage (read SSD disks). A database optimized for IoT will give you the option to offload older data sets into cheaper storage options.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Typically, users are not interestd in a specific data point for a specific device at a specific point of time. They are typically interested in trends, aggregations, time-series data (give me all messages from &lt;em&gt;deviceId01&lt;/em&gt; for the last week). This is a very different access pattern than what most OLTPs and other database systems are optimised for.&lt;/p&gt;

&lt;h3 id=&quot;the-typical-iot-data-access-patterns&quot;&gt;The (typical) IoT data access patterns&lt;/h3&gt;

&lt;p&gt;Typical queries that we see in the field are the following:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Last Values:&lt;/strong&gt; for any specific &lt;em&gt;deviceId&lt;/em&gt;, retrieve the last value sent. Examples of this query are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Get the &lt;em&gt;OperatingHours&lt;/em&gt; of a device. Devices, especially industrial equipment, typically keep a counter of the hours that they have been in continuous operation. For remote monitoring scenarios, you typically care about the value after the equipment’s last reboot, so you are looking to retrieve the last value that device has sent.&lt;/li&gt;
  &lt;li&gt;Remote Monitoring dashboards: these will typically present the user with the &lt;em&gt;current&lt;/em&gt; state of devices, meaning the last value they have transmitted for specific KPIs.&lt;/li&gt;
  &lt;li&gt;Alarms: although the history of alarms is important, users will typically be concerned with the current status of alarms. In particular, they want to know if there are alarms which are currently active, and since when are they active. This is another “get me the last value” scenario.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Time series&lt;/strong&gt; : this is the stereotypical graph that shows the historical values of signals transmitted by a device. This time series can be broken into two types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Raw time series: for example: “&lt;em&gt;for&lt;/em&gt; deviceId01&lt;em&gt;, give me all temperature measurements for the last 24 hours. Visualize the data where the&lt;/em&gt; x &lt;em&gt;axis represents time and the&lt;/em&gt; y &lt;em&gt;axis represents the values&lt;/em&gt;”.&lt;/li&gt;
  &lt;li&gt;Binning: bins allow you to make sense out of very large numbers of time series data points. This is one of the most important features a database must provide to you. Suppose the following ask from a user: “&lt;em&gt;for&lt;/em&gt; deviceId01&lt;em&gt;, give me all temperature measurements for the last 3 months. Visualize the data in a histogram&lt;/em&gt;”. Suppose that &lt;em&gt;deviceId01&lt;/em&gt; is sending temperature measurements every 10 seconds. Asking for the last 3 months of data means retrieving 777’600 data points and throwing them in a dashboard. There is no way any user will be able to make sense out of so much data. Bins allow you to do the following: “&lt;em&gt;for&lt;/em&gt; deviceId01&lt;em&gt;, give me all temperature measurements for the last 3 months. Group the result in bins of 24 hours and, for each of those 24-hour periods, give me the average, mix and max.&lt;/em&gt;” Bins allow the system to summarize the data for the user, and provide projections (like &lt;em&gt;min&lt;/em&gt; and &lt;em&gt;max)&lt;/em&gt; so that the user can identify anomalies in the bins (binning on averages will unfortunately smooth out anomalies and hide them). With this technique, you can show the user a graph with 3 different plots: one for the avg of the bins, one for the max values inside the bins, and one with the min. You can certainly choose other aggregation functions for your bins.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Counters:&lt;/strong&gt; counters are a common aggregation measure. For example: “&lt;em&gt;give me the total number of times that&lt;/em&gt; alarmX &lt;em&gt;was activated for&lt;/em&gt; deviceId01 &lt;em&gt;in the last 30 days&lt;/em&gt;”.&lt;/p&gt;

&lt;p&gt;Of course, your users will throw many other different queries at you. However, the queries mentioned above are the absolute minimum you will need to support, especially if your IoT solution is focused on remote device monitoring. Predictive maintenance scenarios will have different query profiles, usually more sophisticated than the ones mentioned above. So make sure your db has a very rich feature set and query semantics. This will allow you to grow as your IoT solution matures and users ask for more sophisticated capabilities.&lt;/p&gt;

&lt;h2 id=&quot;optimised-for-large-scale-ingress-of-data&quot;&gt;Optimised for large-scale ingress of data&lt;/h2&gt;

&lt;p&gt;IoT can quickly generate millions of messages per day. This obviously depends on the size of your IoT landscape, but in general, you can expect an IoT platform to handle very large message ingress. This can be a challenge for traditional RDBMS systems, since optimizing for &lt;em&gt;writes&lt;/em&gt; often means sacrifing &lt;em&gt;read&lt;/em&gt; performance. Your database must be able to support constant ingress of many messages, yet also provide timely-access to the telemetry data.&lt;/p&gt;

&lt;h2 id=&quot;optimised-for-event-sourcing-pattern&quot;&gt;Optimised for Event-Sourcing pattern&lt;/h2&gt;

&lt;p&gt;IoT lends itself for event sourcing. This is because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You don’t have to change telemetry data. The data that arrives in the database will never be changed. There is no point in changing it. Sensor measurements are what they are at a specific point in time and you’ll never need to change their values. Therefore, your database does not need to support &lt;em&gt;Update&lt;/em&gt; operations on its data. This makes it easier for db vendors to optimise the database for large ingress without compromising the performance of reads. Resource locks are not needed. We call these &lt;em&gt;append-only&lt;/em&gt; databases.&lt;/li&gt;
  &lt;li&gt;Your db does not need to support transactionality. The success or failure of storing a message from &lt;em&gt;deviceA&lt;/em&gt; does not depend on the success of storing any other message. IoT telemetry messages are independent from each other and you don’t have to implement transactionality when storing them. Eliminating this constraint also gives db vendors the freedom to make design choices that facilitate large data ingress and fast query performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So databases which are optimised for transactionality, atomicity, editability are typically poor choices for IoT scenarios. This is because implementing these features  forces the db vendor to make important compromises which will affect performance.&lt;/p&gt;

&lt;h2 id=&quot;offered-as-a-managed-service&quot;&gt;Offered as a managed service&lt;/h2&gt;

&lt;p&gt;This might be a contentious topic, specially for those of you skeptic of the public cloud. Yet, I would never advise a customer to deploy a database for IoT on-prem, particularly if you’re expecting a very large estate of IoT devices. Taking a database from a cloud provider will save you months in terms of time-to-market, as you won’t have to design the networking, compute and storage for a large on-prem db cluster. You can get all these from a cloud vendor in a matter of minutes. Sizing the infrastructure components of a high-scale, distributed database can be quite hard (think about designing to provide the right disk I/O latency. Not an easy problem to solve).&lt;/p&gt;

&lt;h2 id=&quot;distributed-architecture&quot;&gt;Distributed architecture&lt;/h2&gt;

&lt;p&gt;Due to the large scale of data ingress, and the complexity of time-series queries, your db must be able to scale horizontally across multiple nodes. This already renders many traditional RDBMS systems ineligible, as they commonly find it very hard to implement cost-effective distributed architectures.&lt;/p&gt;

&lt;h2 id=&quot;be-flexible-with-schemas&quot;&gt;Be flexible with schemas&lt;/h2&gt;

&lt;p&gt;Your IoT devices will have many different types of messages. You will have to support multiple generations or families of devices, each one having its own particular data model. Reaching a common data model is extremely hard to do, as that typically requires you to deploy changes in the software of your existing field devices to accomodate data model standardisation. This is impractical and expensive. Your IoT db does not have to be strictly schemaless, but it should be able to work with flexible data structures on the fly.&lt;/p&gt;

&lt;h2 id=&quot;provide-deduplication&quot;&gt;Provide deduplication&lt;/h2&gt;

&lt;p&gt;When designing IoT systems, you should assume &lt;em&gt;at-least-once&lt;/em&gt; delivery of IoT messages from your devices. You must be prepared to handle message duplication. The transmision of messages from your devices might fail due to transient errors. Devices might retry to send messages that were already sent to the backend. Your database should have operators to retrieve only distinct messages. This is typically done by having devices transmit every message with a &lt;em&gt;messageId&lt;/em&gt; property (we use a  UUID value for that purpose), so the db can filter for distinctiveness.&lt;/p&gt;

&lt;h2 id=&quot;test-drive&quot;&gt;Test Drive&lt;/h2&gt;

&lt;p&gt;To bring these recommendations to practice, this blog series will focus on two databases which are optimised for time series analysis, and quite common on IoT scenarios: &lt;a href=&quot;https://azure.microsoft.com/en-us/services/data-explorer/&quot;&gt;Azure Data Explorer (ADX)&lt;/a&gt; and &lt;a href=&quot;https://www.timescale.com&quot;&gt;Timescale&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I have excluded InfluxDb because I can’t find a cloud-native managed service for it. I have also excluded AWS Timestream as I don’t have access to it on a trial account. I will also not cover Druid since I lack the technical knowledge to evaluate it properly. However, all the code used to run my experiments on Timescale and ADX is available on &lt;a href=&quot;https://github.com/murdockcrc/timeseries-loader&quot;&gt;GitHub&lt;/a&gt;. I highly encourage AWS, Influx and Druid specialists to run the same experiments on those platforms and report the results on the Disqus comments section. I’ll extend the contents of this blog series with your findings too (with author credit, obviously).&lt;/p&gt;

&lt;p&gt;Why did I choose ADX and Timescale? I have been working with ADX for almost 18 months. I have been working with it even before it was called Azure Data Explorer, and even before it was an Azure service available for preview. I have had the pleasure of working closely with the Microsoft ADX engineering team, and I have witnessed first hand how powerful this database is.&lt;/p&gt;

&lt;p&gt;Why Timescale? Timescale is an extension based on the popular PostgreSQL database. Most of it is open source. Timescale counts very large production deployments under its belt, and has produced fantastic results in benchmark analysis on both writes/sec and complex query performance. Whereas ADX is a Microsoft-propietary database available only in the Azure cloud, Timescale on PostgreSQL is OSS portable across clouds (or even on-prem). It is important for me to also offer the open-source flavor to this analysis.&lt;/p&gt;

&lt;p&gt;This is a blog series. In the second post, I’ll focus on the process of ingesting data into both databases.&lt;/p&gt;

&lt;p&gt;In the third and final post, I’ll focus on the process of querying the data based on the data access patterns specified above.&lt;/p&gt;

&lt;p&gt;Stay tuned.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Comment below if you think there is another key database requirement missing from the list above.&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><category term="iot" /><category term="azure" /><summary type="html">This is a blog post series focused on IoT</summary></entry><entry><title type="html">Using IoT to help our Marketing and Sales friends</title><link href="http://localhost:4000/2019/04/12/using-iot-to-help-our-marketing-and-sales-friends.html" rel="alternate" type="text/html" title="Using IoT to help our Marketing and Sales friends" /><published>2019-04-12T20:14:00+02:00</published><updated>2019-04-12T20:14:00+02:00</updated><id>http://localhost:4000/2019/04/12/using-iot-to-help-our-marketing-and-sales-friends</id><content type="html" xml:base="http://localhost:4000/2019/04/12/using-iot-to-help-our-marketing-and-sales-friends.html">&lt;p&gt;For the last 3 years, IoT has been a term hyped to no end by the industry and the media (together with Big Data and Blockchain, amongst others). Hyped technology concepts are good because they create awareness outside of its starting niche community, but it also has the drawback of creating skepticism. IoT is no exception to this phenomenon, especially since many organization have invested significant amount of capital in the technology without delivering value back to their organisations.&lt;/p&gt;

&lt;p&gt;So today, I want to present a real, &lt;strong&gt;tangible&lt;/strong&gt; case of how we used IoT to create value for marketing and sales. Due to contractual restrictions, I won’t name the customers, but we’ve done this twice at with customers.&lt;/p&gt;

&lt;h2 id=&quot;how-equipment-manufacturers-make-money&quot;&gt;How equipment manufacturers make money&lt;/h2&gt;

&lt;p&gt;The scenarios involve industrial equipment manufacturers.  This equipment is static, that is, it rarely changes its physical location.&lt;/p&gt;

&lt;p&gt;Typically, manufacturers produce the equipment, which is then bought by a &lt;em&gt;reseller&lt;/em&gt;. The reseller then takes the equipment to a geographical location of its competence (a sales territory), sells it there to the end-customer, and provides after-sales support and services. Think about this as the way you purchase a Toyota. A dealer (who is responsible for a specific geography), imports the car into its territoty, sales it to the consumer, and provides support and warranty services. Under this model, the manufacturer does not even know who the end customer is. This _indirect distribution channel_is very common among equipment manufacturers. The manufacturer only knows it sold 100’000 devices to the Japanese reseller, but does not know more than that.&lt;/p&gt;

&lt;h2 id=&quot;the-value-of-equipment-location&quot;&gt;The value of equipment location&lt;/h2&gt;

&lt;p&gt;Wouldn’t it be nice if our manufacturer could know the exact location of its equipment? It turns out that this information is tremendously valuable for marketing and sales purposes.&lt;/p&gt;

&lt;p&gt;If we know, for instance, that out of these 100’000 equipment sold in Japan, most of them are sold in Osaka, you would be able to create targeted, data-driven marketing decisions to support and establish a position in that specific market. Without this data, you might be tempted to assume that Tokyo is your biggest market, but maybe it isn’t. So instead of organizing a brand-specific event in Tokyo, using equipment location data you would decide to invest your marketing money where your most loyal customers are: Osaka.&lt;/p&gt;

&lt;p&gt;What about sales? If we could determine the location of the equipment, we could monitor the trend of on-boarding and off-boardings on a per-sales district basis. We can spot trends in activations: if we detect that on-boardings of devices in Osaka haven been declining for the last 3 consecutive months, you will have the evidence to prove that your Osaka market is under attack from the competition (perhaps your local reseller got a better deal from a competitor and is pushing their product instead of yours). Without this data, your Japanese sales manager will never be able to react in time to this sales assault, your sales guys are basically flying in the dark and being blindsided by a disloyal reseller.&lt;/p&gt;

&lt;p&gt;From the technical perspective, the two use cases I have just presented may seem very trivial. Yet, they are of enormous value to your sales and marketing managers. Knowing where your devices are physically located will allow you to make data-driven marketing decision (instead of speculation or gut-feeling). This capability will also allow you to keep a tight, near-real time pulse on your sales dynamics, allowing you to promptly react to real (instead of &lt;em&gt;perceived&lt;/em&gt;) threats.&lt;/p&gt;

&lt;h2 id=&quot;how-do-we-get-there&quot;&gt;How do we get there&lt;/h2&gt;

&lt;p&gt;To achieve this capability, our implementations have typically looked like this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Your devices must have a connectivity capability. How you achieve this is irrelevant, but your devices must be able to connect to the Internet and to Microsoft’s Azure IoT Hub.&lt;/li&gt;
  &lt;li&gt;Once your devices are able to establish a network connection to your IoT Hub, we will configure the Hub to publish &lt;em&gt;Connect/Disconnect&lt;/em&gt; events to us. This means that, every time any of your devices connects or disconnects, we will receive an automatic notification from the Hub. This capability was added this year by Microsoft, and you can read more about it &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/iot-hub/tutorial-use-metrics-and-diags&quot;&gt;HERE&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We implemented a process that listens to those &lt;em&gt;Connect/Disconnect_events in batch mode (every x minutes). These events have  a specific &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-monitor-resource-health#understand-the-logs&quot;&gt;schema&lt;/a&gt;, and publish multiple data points. One of them is the _maskedClientIpAddress&lt;/em&gt;. Microsoft will disclose to use the public client IP address the device used to connect to the Hub. However, Microsoft will not disclose the last octet of the IP address. This is to protect the privacy of the end client. However, this is good enough for our purposes. Here’s how these events look like:
&lt;!--kg-card-begin: markdown--&gt;&lt;/p&gt;

    &lt;p&gt;{
  	“records”: 
  	[
          {
              “time”: “ UTC timestamp”,
              “resourceId”: “Resource Id”,
              “operationName”: “deviceConnect”,
              “category”: “Connections”,
              “level”: “Information”,
              “properties”: “{&quot;deviceId&quot;:&quot;&lt;deviceId&gt;\&quot;,\&quot;protocol\&quot;:\&quot;&lt;protocol&gt;\&quot;,\&quot;authType\&quot;:\&quot;{\\\&quot;scope\\\&quot;:\\\&quot;device\\\&quot;,\\\&quot;type\\\&quot;:\\\&quot;sas\\\&quot;,\\\&quot;issuer\\\&quot;:\\\&quot;iothub\\\&quot;,\\\&quot;acceptingIpFilterRule\\\&quot;:null}\&quot;,\&quot;maskedIpAddress\&quot;:\&quot;&lt;maskedIpAddress&gt;\&quot;}&quot;,
              &quot;location&quot;: &quot;Resource location&quot;
          }
      ]
  }&lt;/maskedIpAddress&gt;&lt;/protocol&gt;&lt;/deviceId&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!--kg-card-end: markdown--&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Once we get the masked IP address from the Hub (together with the device Id, which is also part of the message we get from the Hub), we send that IP address to a geo location service. IP geolocation is a third-party service which transforms a public IP address into a geographical location, like this:
&lt;!--kg-card-begin: markdown--&gt;&lt;/p&gt;

    &lt;p&gt;{
      “status”:”success”,
      “description”:”Data successfully received.”,
      “data”:{
          “geo”:{
              “host”:”www.google.com”,
              “ip”:”74.125.29.147”,
              “rdns”:”qg-in-f147.1e100.net”,
              “asn”:”AS15169”,
              “isp”:”Google Inc.”,
              “country_name”:”United States”,
              “country_code”:”US”,
              “region_name”:”California”,
              “region_code”:”CA”,
              “city”:”Mountain View”,
              “postal_code”:”94043”,
              “continent_name”:”North America”,
              “continent_code”:”NA”,
              “latitude”:”37.419200897217”,
              “longitude”:”-122.05740356445”,
              “metro_code”:”650”,
              “timezone”:”America\/Los_Angeles”,
              “datetime”:”2015-05-22 09:10:35”
          }
      }
  }&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;In the sample above, we used a geolocation service to transform the public IP address _74.125.29.147_into a physical location. For our purposes, we don’t need to know an exact address. The city, municipality or district is good enough.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Once we get this information from the service, we save the data to the device’s digital twin, allowing us to make it visible for historical analysis to our sales and marketing friends.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We implement this as a fully-automated process using an Azure Logic App. So basically, you can achieve this location-gathering capability with pretty much zero programming skills (except for the connectivity of your devices to the Hub, which does require some effort from your device engineering team).&lt;/p&gt;

&lt;p&gt;Since we are storing this information in our own database, we can then use the power of Azure Maps to create &lt;a href=&quot;https://channel9.msdn.com/Shows/Internet-of-Things-Show/Heat-Maps-and-Image-Overlays-in-Azure-Maps&quot;&gt;heatmaps&lt;/a&gt;for marketing and sales. &lt;em&gt;Business people love maps!&lt;/em&gt; And they love it for a reason: it allows them to boil down large amounts of data into simple visualisations which are easy to interpret, act on and share.&lt;/p&gt;

&lt;h2 id=&quot;caveats&quot;&gt;Caveats&lt;/h2&gt;

&lt;p&gt;I’ve found this solution to be very powerful and valuable to our customers. However, there are some caveats to be aware of:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;You depend on your end-customer providing connectivity to your equipment. Typically, manufacturers enable this networking by leveraging the end customer’s WLAN/LAN or using a bluetooth connection to a smart phone app. However, this assumes the end customer will onboard the device into his network. If he doesn’t do that, you’ll never get any data. The alternative to that is to embed a connectivity module into your hardware and have it transmit data autonomously. However, this increases the production cost of your device.&lt;/li&gt;
  &lt;li&gt;Since Microsoft does not disclose the last octet of the client IP address, we need to make an assumption and substitute the “xxx” in the last octet with any other number between 1 and 254. This change may throw off the accuracy of the geolocation service. In practice though, since knowing the city where the device is connecting from is typically good enough for our objectives, this is not a real problem.&lt;/li&gt;
  &lt;li&gt;If the users of your equipment are large B2B organizations, bear in mind that the actual exit point to the Internet might be very far away from the actual location of the equipment (typical case are satellite offices connected via an MPLS network and using a centralized Internet exit point). If these are edge cases within your customer base, then you are good to go.&lt;/li&gt;
  &lt;li&gt;Resellers can become very skeptical of you once they know you are trying to learn more about their end-customer. From a business perspective, this is a tricky balance to strike: you don’t want to depend so much on resellers and you want to know who your customer is. At the same time, resellers are the ones that bring money to you today and you need to protect that relationship.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;What have we achieved so far? We know the location of the equipment we are selling. But we still don’t know who the end customer is. We don’t know how often they use the equipment, or for what purposes. We still don’t know how often the equipment fails (remember, after-sales support is provided by the reseller. They will fix equipment that broke down and very rarely do they tell you about that). Essentially, we still know &lt;strong&gt;nothing&lt;/strong&gt; about the actual buyer of our equipment. We are still so very far away from knowing anything about the end customer.&lt;/p&gt;

&lt;p&gt;But this little piece of information we do now know (the equipment’s location) is so very powerful. We removed speculation from marketing and allow the department to make data-driven decisions based on geographical facts about equipment location. We allow sales to closely monitor the sales estate on a sales district basis. We are able to automatically keep a pulse on our equipment and its location, and spot trends in sales. These trends allow us to identify threats, disloyal resellers, the effectiveness of sales campaigns on specific locations, and bring data to the sales management process.&lt;/p&gt;</content><author><name></name></author><category term="iot" /><category term="digitalization" /><summary type="html">For the last 3 years, IoT has been a term hyped to no end by the industry and the media (together with Big Data and Blockchain, amongst others). Hyped technology concepts are good because they create awareness outside of its starting niche community, but it also has the drawback of creating skepticism. IoT is no exception to this phenomenon, especially since many organization have invested significant amount of capital in the technology without delivering value back to their organisations.</summary></entry><entry><title type="html">How to serialise your IoT messages</title><link href="http://localhost:4000/2019/04/02/how-to-serialise-your-iot-messages.html" rel="alternate" type="text/html" title="How to serialise your IoT messages" /><published>2019-04-02T20:16:00+02:00</published><updated>2019-04-02T20:16:00+02:00</updated><id>http://localhost:4000/2019/04/02/how-to-serialise-your-iot-messages</id><content type="html" xml:base="http://localhost:4000/2019/04/02/how-to-serialise-your-iot-messages.html">&lt;p&gt;One of the first questions that pops up during our IoT engagements is: &lt;em&gt;how should we serialise messages from our IoT devices?&lt;/em&gt; Althoughnobody has ever questioned serializing messages using a JSON structure, it is not always clear what schema those JSON messages should take.&lt;/p&gt;

&lt;p&gt;We typically recommend customers to not reinvent the wheel, and this time is no exception.  &lt;a href=&quot;https://cloudevents.io/&quot;&gt;CloudEvents&lt;/a&gt;and adopt it as the data structure to serialize their IoT messages.&lt;/p&gt;

&lt;h2 id=&quot;why-cloudevents&quot;&gt;Why CloudEvents?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;It is a project under the stewarship of the Cloud Native Computing Foundation. On top of that, the working group for CloudEvents has members from some of the world’s heaviest weights in computing, including Microsoft, Alibaba, Amazon Web Services, Google and &lt;a href=&quot;https://github.com/cloudevents/spec/blob/master/community/contributors.md&quot;&gt;many others&lt;/a&gt;. This gives you certainty that the project stands on solid technical grounds. These providers have already started adopting the specification, which means you can benefit from out-of-the-box features too (see how Azure Event Grid &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/event-grid/cloudevents-schema&quot;&gt;already supports&lt;/a&gt;CloudEvents natively).&lt;/li&gt;
  &lt;li&gt;CloudEvents offers support for transport protocols bindings which are common in the IoT world, such as &lt;a href=&quot;https://github.com/cloudevents/spec/blob/v0.2/amqp-transport-binding.md&quot;&gt;AMQP&lt;/a&gt;, &lt;a href=&quot;https://github.com/cloudevents/spec/blob/v0.2/mqtt-transport-binding.md&quot;&gt;MQTT&lt;/a&gt;and &lt;a href=&quot;https://github.com/cloudevents/spec/blob/v0.2/http-transport-binding.md&quot;&gt;HTTP&lt;/a&gt;transports.&lt;/li&gt;
  &lt;li&gt;There are &lt;a href=&quot;https://github.com/cloudevents/spec/blob/master/SDK.md&quot;&gt;multiple SDKs&lt;/a&gt;already offered in many languages, which will speed up your project as you don’t have to write your own serialization libraries.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The CloudEvents project also provides a spec for &lt;a href=&quot;https://github.com/cloudevents/spec/blob/v0.2/json-format.md&quot;&gt;JSON messages&lt;/a&gt;. This is probably the core asset of the project. In my experience, agreeing on a common data model early on in your IoT project is key. Yet it is quite difficult to reach this agreement. Getting this understanding with your device engineering team, your cloud development team and your data science team is essential to avoid unnecessary refactoring, confusion and frustration between the teams.&lt;/p&gt;

&lt;p&gt;The data model you choose should be flexible to accommodate your multiple device types, their variations and their different generations. Still, it should also enforce a data structure that will allow all players in the game to process and interpret messages in a coherent way. In essence, these two requirements contradict each other and it is hard to achieve the correct balance between them. This is perhaps the trickiest point in trying to agree on a data model. With CloudEvents, I recommend that you try to take the spec as-is, as this will allow you to benefit from all the artifacts and integrations the project will deliver over time. However, if you do find in your project a blocker which does not allow you to fully embrace the CloudEvents spec, then I suggest you still take a look at it as a source of inspiration for your own message models.&lt;/p&gt;

&lt;p&gt;For illustration, here is a variation of a data model which we used in the past. Some values (property names in the JSON body) have been modified for anonymity.&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
    &quot;specversion&quot; : &quot;0.2&quot;,
    &quot;type&quot; : &quot;com.customerId.iot.deviceType.eventType&quot;,
    &quot;source&quot; : &quot;device_id&quot;,
    &quot;id&quot; : &quot;B234-1234-1234&quot;,
    &quot;time&quot; : &quot;2018-04-05T17:31:00Z&quot;,
    &quot;customer_extension_01&quot; : &quot;value&quot;,
    &quot;customer_extension_02&quot; : {
        &quot;otherValue&quot;: 5
    },
    &quot;contenttype&quot; : &quot;application/json&quot;,
    &quot;data&quot; : [
        {
            &quot;signal_name_01&quot;: &quot;temp_sensor_01&quot;,
            &quot;values&quot;: [24.01]
        },
        {
            &quot;signal_name_02&quot;: &quot;temp_sensor_02&quot;,
            &quot;values&quot;: [25.02]
        },
        {
            &quot;signal_name_03&quot;: &quot;alarms_controller&quot;,
            &quot;values&quot;: [true, false, false, true]
        }
    ]    
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;Note that, as per the spec, the _data_property is simply a native &lt;a href=&quot;https://tools.ietf.org/html/rfc7159#section-4&quot;&gt;JSON object&lt;/a&gt;, so you have the flexibility to represent whatever data model and hierarchies you need. Typically, it is a good idea to stick with an array of JSON objects, each object having the same data structure as the rest to avoid complex parsing rules. With the example message above, you can add event-based signals, polled-based signals, alarms or notifications to your message, and be able to keep more or less the same structure across the board. It’s a quite versatile data structure.&lt;/p&gt;

&lt;p&gt;Furthermore, this data structure is good for devices sending only one signal (with an array of 1 element) or for devices sending thousands of signals in the same message.&lt;/p&gt;

&lt;p&gt;The structure also gives you the option to add &lt;em&gt;extensions&lt;/em&gt;(the properties called &lt;em&gt;customer_extension_xx&lt;/em&gt;), meaning you can extend the message as it flows through your pipeline (for example, add a _field gateway timestamp_as the message flows through your IoT gateway).&lt;/p&gt;

&lt;h2 id=&quot;challenges-of-the-approach-and-their-mitigation&quot;&gt;Challenges of the approach and their mitigation&lt;/h2&gt;

&lt;p&gt;I’ve often heard this approach be critized as being too verbose, and thus inefficient in terms of bandwidth costs. For example, each object in the &lt;em&gt;data_array has the property names _signal_name_&lt;/em&gt; and _values_repeated over and over again, possibly even thousands of times for very large messages. This is a valid concern, which can be mitigated in the following ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The impact of this verbosity will only be felt if your IoT project reaches very large  scale. For projects just getting off the ground, this verbosity will not be an issue.&lt;/li&gt;
  &lt;li&gt;You can serialize your JSON messages at the edge using something like MessagePack, which will significantly reduce the size of your messages.&lt;/li&gt;
  &lt;li&gt;You can use compression on your messages, such as GZip/Deflate.&lt;/li&gt;
  &lt;li&gt;You can combine the two options above (MessagePack + Compression), achieving very attractive compression ratios.&lt;/li&gt;
  &lt;li&gt;All of the above has the purpose of enabling you to keep a &lt;strong&gt;self-descriptive, semantically-rich&lt;/strong&gt; data structure, while at the same time achieving efficient transmission costs. Trust me, you want your messages to be self-descriptive, otherwise you’ll fall in model-version-management hell and that is a problem whose complexity will erase the savings you achieved in bandwidth costs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;test-drive&quot;&gt;Test Drive&lt;/h2&gt;

&lt;p&gt;Let’s take this approach for a test drive. We’ll take the above JSON message as a benchmark. We’ll start by serializing that JSON string into a &lt;em&gt;byte[]&lt;/em&gt;, then we’ll serialize that byte array with MessagePack, and we’ll finally use GZIP to compress the MessagePacke message. We’ll compare the size of each byte array, and then we’ll deserialize the whole thing back to a _string_to confirm the correctness of data.&lt;/p&gt;

&lt;p&gt;Here’s the code to execute the experiment:&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;using System;
using System.Text;
using MessagePack;
using System.IO.Compression;
using System.IO;

namespace compress
{
    class Program
    {
        private static string _messageSample = @&quot;
            {
                &quot;&quot;specversion&quot;&quot; : &quot;&quot;0.2&quot;&quot;,
                &quot;&quot;type&quot;&quot; : &quot;&quot;com.customerId.iot.deviceType.eventType&quot;&quot;,
                &quot;&quot;source&quot;&quot; : &quot;&quot;device_id&quot;&quot;,
                &quot;&quot;id&quot;&quot; : &quot;&quot;B234-1234-1234&quot;&quot;,
                &quot;&quot;time&quot;&quot; : &quot;&quot;2018-04-05T17:31:00Z&quot;&quot;,
                &quot;&quot;customer_extension_01&quot;&quot; : &quot;&quot;value&quot;&quot;,
                &quot;&quot;customer_extension_02&quot;&quot; : {
                    &quot;&quot;otherValue&quot;&quot;: 5
                },
                &quot;&quot;contenttype&quot;&quot; : &quot;&quot;application/json&quot;&quot;,
                &quot;&quot;data&quot;&quot; : [
                    {
                        &quot;&quot;signal_name_01&quot;&quot;: &quot;&quot;temp_sensor_01&quot;&quot;,
                        &quot;&quot;values&quot;&quot;: [24.01]
                    },
                    {
                        &quot;&quot;signal_name_02&quot;&quot;: &quot;&quot;temp_sensor_02&quot;&quot;,
                        &quot;&quot;values&quot;&quot;: [25.02]
                    },
                    {
                        &quot;&quot;signal_name_03&quot;&quot;: &quot;&quot;alarms_controller&quot;&quot;,
                        &quot;&quot;values&quot;&quot;: [true, false, false, true]
                    }
                ]    
            }&quot;;

        static void Main(string[] args)
        {
            var messageInBytes = Encoding.UTF8.GetBytes(_messageSample);
            Console.WriteLine($&quot;Plain JSON object size: {messageInBytes.Length}&quot;);
            
            var messageAsMessagePack = MessagePackSerializer.FromJson(_messageSample);
            Console.WriteLine($&quot;MessagePack object size: {messageAsMessagePack.Length}&quot;);

            using (FileStream fileStream = File.Create(&quot;compressed.bin&quot;))
            using (GZipStream compressionStream = new GZipStream(fileStream, CompressionMode.Compress))
            {
                var memStream = new MemoryStream(messageAsMessagePack);
                memStream.CopyTo(compressionStream);
            }

            using (FileStream fileStream = File.Open(&quot;compressed.bin&quot;, FileMode.Open))
            using (FileStream decompressedfile = File.Create(&quot;decompressed.txt&quot;))
            using (GZipStream decompressStream = new GZipStream(fileStream, CompressionMode.Decompress))
            {
                decompressStream.CopyTo(decompressedfile);
            }     

            using (FileStream fileStream = File.Open(&quot;decompressed.txt&quot;, FileMode.Open))
            {
                var deserialized = MessagePackSerializer.Deserialize&amp;lt;object&amp;gt;(fileStream);
                Console.WriteLine(MessagePackSerializer.ToJson(deserialized));
            }                                             
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;This code produces the following output:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Plain JSON object size: 1004MessagePack object size: 400{“specversion”:”0.2”,”type”:”com.customerId.iot.deviceType.eventType”,”source”:”device_id”,”id”:”B234-1234-1234”,”time”:”2018-04-05T17:31:00Z”,”customer_extension_01”:”value”,”customer_extension_02”:{“otherValue”:5},”contenttype”:”application/json”,”data”:[{“signal_name_01”:”temp_sensor_01”,”values”:[24.01]},{“signal_name_02”:”temp_sensor_02”,”values”:[25.02]},{“signal_name_03”:”alarms_controller”,”values”:[true,false,false,true]}]}&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Furthermore, the size of the compressed stream is now reduced to 293 bytes.&lt;/p&gt;

&lt;p&gt;Our original message had a size of 1004 bytes. Using MessagePack and GZIP compression, we have shrunk that down to a mere 293 bytes! And we still have a semantically-rich, self-descriptive message, while achieving significant bandwidth costs reduction using efficient serialization and compression. SWEET!&lt;/p&gt;

&lt;figure class=&quot;kg-card kg-image-card&quot;&gt;&lt;img src=&quot;/content/images/2019/04/image.png&quot; class=&quot;kg-image&quot; /&gt;&lt;/figure&gt;</content><author><name></name></author><category term="iot" /><summary type="html">One of the first questions that pops up during our IoT engagements is: how should we serialise messages from our IoT devices? Althoughnobody has ever questioned serializing messages using a JSON structure, it is not always clear what schema those JSON messages should take.</summary></entry><entry><title type="html">Adding custom headers to your IoT Hub messages (via HTTP REST endpoint)</title><link href="http://localhost:4000/2018/07/28/adding-custom-headers-to-your-iot-hub-messages-via-http-rest-endpoint.html" rel="alternate" type="text/html" title="Adding custom headers to your IoT Hub messages (via HTTP REST endpoint)" /><published>2018-07-28T09:42:00+02:00</published><updated>2018-07-28T09:42:00+02:00</updated><id>http://localhost:4000/2018/07/28/adding-custom-headers-to-your-iot-hub-messages-via-http-rest-endpoint</id><content type="html" xml:base="http://localhost:4000/2018/07/28/adding-custom-headers-to-your-iot-hub-messages-via-http-rest-endpoint.html">&lt;p&gt;&lt;em&gt;Please do yourself a favor and try to always work with IoT Hub’s client SDK’s.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, if for some reason you are forced to work with the REST endpoints, here is a tip to include custom message properties into your IoT Hub messages.&lt;/p&gt;

&lt;h2 id=&quot;how-to-add-custom-message-properties&quot;&gt;How to add custom message properties&lt;/h2&gt;

&lt;p&gt;To send device-to-cloud (D2C) messages to IoT Hub via the HTTP REST endpoint, you need to POST to the &lt;code class=&quot;highlighter-rouge&quot;&gt;http://{your_iothub_fqdn}/devices/{device_id}/events/messages&lt;/code&gt; endpoint.&lt;/p&gt;

&lt;p&gt;To add a custom header, simply add an HTTP header prefixed with &lt;code class=&quot;highlighter-rouge&quot;&gt;iot-app-&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sample request:&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;kg-card kg-image-card&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*O-PWO8QBGZPZZ0kF2dhyIg.png&quot; class=&quot;kg-image&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;kg-card kg-image-card&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*hM_82MHSdGbOmQLko9OoZQ.png&quot; class=&quot;kg-image&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;You will find the custom properties in the &lt;code class=&quot;highlighter-rouge&quot;&gt;applicationProperties&lt;/code&gt; property of the IoT Hub message.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why are message properties useful?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you send a message to IoT Hub, in principle, what you are sending is a byte array of information. Sometimes you need to add properties to your messages to facilitate processing. For instance, you might want to tag a message as being an &lt;code class=&quot;highlighter-rouge&quot;&gt;alarm&lt;/code&gt;, and such messages will get treated with priority. If you are sending your messages in plain JSON, you could simply stick that additional property in the JSON body. But maybe you don’t want to do that, because:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;You don’t want to “pollute” your messages with additional properties, which by nature, don’t belong in the message body.&lt;/li&gt;
  &lt;li&gt;You don’t want to unwrap the message to read these properties (that slows the ingestion pipeline down).&lt;/li&gt;
  &lt;li&gt;You can’t add such properties to the message because you are not sending JSON to IoT Hub (you might be sending compressed payloads, batched messages, Avro-encoded messages, XML, or raw byte arrays).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Another use case for properties is the use of IoT Hub routing rules. Although IoT Hub recently added the possibility of making routing decisions based on the message’s body, we don’t advise customers to do this for the following reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;This forces IoT Hub to deserialize your payload, which adds latency to the message ingestion process.&lt;/li&gt;
  &lt;li&gt;This won’t work unless your messages are all serialized in plain JSON.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><category term="iot" /><summary type="html">Please do yourself a favor and try to always work with IoT Hub’s client SDK’s.</summary></entry></feed>